{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Auto Assembler\n",
    "\n",
    "This tool is meant to streamline creation of single module packages.\n",
    "Its purpose is to automate as many aspects of python package creation as possible,\n",
    "to shorten a development cycle of reusable components, maintain certain standard of quality\n",
    "for reusable code. It provides tool to simplify the process of package creatrion\n",
    "to a point that it can be triggered automatically within ci/cd pipelines,\n",
    "with minimal preparations and requirements for new modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from python_modules.package_auto_assembler import (VersionHandler, \\\n",
    "    ImportMappingHandler, RequirementsHandler, MetadataHandler, \\\n",
    "        LocalDependaciesHandler, SetupDirHandler, PackageAutoAssembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples contain: \n",
    "1. package versioning\n",
    "2. import mapping\n",
    "3. extracting and merging requirements\n",
    "4. preparing metadata\n",
    "5. merging local dependacies into single module\n",
    "6. assembling setup directory\n",
    "7. making a package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Package versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize VersionHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = VersionHandler(\n",
    "    # required\n",
    "    versions_filepath = '../tests/package_auto_assembler/lsts_package_versions.yml',\n",
    "    log_filepath = '../tests/package_auto_assembler/version_logs.csv',\n",
    "    # optional \n",
    "    default_version = \"0.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv.add_package(\n",
    "    package_name = \"new_package\", \n",
    "    # optional\n",
    "    version = \"0.0.1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are no known versions of 'another_new_package', 0.0.1 will be used!\n"
     ]
    }
   ],
   "source": [
    "pv.increment_patch(\n",
    "    package_name = \"new_package\"\n",
    ")\n",
    "## for not tracked package\n",
    "pv.increment_patch(\n",
    "    package_name = \"another_new_package\",\n",
    "    # optional\n",
    "    default_version = \"0.0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display current versions and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'another_new_package': '0.0.1', 'new_package': '0.0.2'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv.get_versions(\n",
    "    # optional\n",
    "    versions_filepath = 'lsts_package_versions.yml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv.get_version(\n",
    "    package_name='new_package'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Package</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 21:09:37</td>\n",
       "      <td>new_package</td>\n",
       "      <td>0.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 21:09:39</td>\n",
       "      <td>new_package</td>\n",
       "      <td>0.0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 21:09:39</td>\n",
       "      <td>another_new_package</td>\n",
       "      <td>0.0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp              Package Version\n",
       "0  2024-01-01 21:09:37          new_package   0.0.1\n",
       "1  2024-01-01 21:09:39          new_package   0.0.2\n",
       "2  2024-01-01 21:09:39  another_new_package   0.0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv.get_logs(\n",
    "    # optional\n",
    "    log_filepath = 'version_logs.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flush versions and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv.flush_versions()\n",
    "pv.flush_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize ImportMappingHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = ImportMappingHandler(\n",
    "    # required\n",
    "    mapping_filepath = \"../env_spec/package_mapping.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load package mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIL': 'Pillow',\n",
       " 'bs4': 'beautifulsoup4',\n",
       " 'fitz': 'PyMuPDF',\n",
       " 'attr': 'attrs',\n",
       " 'dotenv': 'python-dotenv',\n",
       " 'googleapiclient': 'google-api-python-client',\n",
       " 'sentence_transformers': 'sentence-transformers',\n",
       " 'flask': 'Flask',\n",
       " 'stdlib_list': 'stdlib-list',\n",
       " 'sklearn': 'scikit-learn',\n",
       " 'yaml': 'pyyaml'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.load_package_mappings(\n",
    "    # optional\n",
    "    mapping_filepath = \"../env_spec/package_mapping.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting and merging requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize RequirementsHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = RequirementsHandler(\n",
    "    # optional/required later\n",
    "    module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\",\n",
    "    package_mappings = {'PIL': 'Pillow',\n",
    "                        'bs4': 'beautifulsoup4',\n",
    "                        'fitz': 'PyMuPDF',\n",
    "                        'attr': 'attrs',\n",
    "                        'dotenv': 'python-dotenv',\n",
    "                        'googleapiclient': 'google-api-python-client',\n",
    "                        'sentence_transformers': 'sentence-transformers',\n",
    "                        'flask': 'Flask',\n",
    "                        'stdlib_list': 'stdlib-list',\n",
    "                        'sklearn': 'scikit-learn',\n",
    "                        'yaml': 'pyyaml'},\n",
    "    requirements_output_path = \"../tests/package_auto_assembler/\",\n",
    "    output_requirements_prefix = \"requirements_\",\n",
    "    custom_modules_filepath = \"../tests/package_auto_assembler/dependancies\",\n",
    "    python_version = '3.8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List custom modules for a given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comparisonframe', 'shouter']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh.list_custom_modules(\n",
    "    # optional\n",
    "    custom_modules_filepath=\"../tests/package_auto_assembler/dependancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if module is a standard python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh.is_standard_library(\n",
    "    # required\n",
    "    module_name = 'shouter',\n",
    "    # optional\n",
    "    python_version = '3.8'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract requirements from the module file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### mock_vector_database.py',\n",
       " 'numpy',\n",
       " 'dill==0.3.7',\n",
       " 'attrs>=22.2.0',\n",
       " 'requests==2.31.0',\n",
       " 'hnswlib==0.7.0',\n",
       " 'sentence-transformers==2.2.2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh.extract_requirements(\n",
    "    # optional\n",
    "    module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\",\n",
    "    custom_modules = ['comparisonframe', 'shouter'],\n",
    "    package_mappings = {'PIL': 'Pillow',\n",
    "                        'bs4': 'beautifulsoup4',\n",
    "                        'fitz': 'PyMuPDF',\n",
    "                        'attr': 'attrs',\n",
    "                        'dotenv': 'python-dotenv',\n",
    "                        'googleapiclient': 'google-api-python-client',\n",
    "                        'sentence_transformers': 'sentence-transformers',\n",
    "                        'flask': 'Flask',\n",
    "                        'stdlib_list': 'stdlib-list',\n",
    "                        'sklearn': 'scikit-learn',\n",
    "                        'yaml': 'pyyaml'},\n",
    "    python_version = '3.8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save requirements to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.write_requirements_file(\n",
    "    # optional/required later\n",
    "    module_name = 'mock_vector_database',\n",
    "    requirements = ['### mock_vector_database.py',\n",
    "                    'numpy',\n",
    "                    'dill==0.3.7',\n",
    "                    'attrs>=22.2.0',\n",
    "                    'requests==2.31.0',\n",
    "                    'hnswlib==0.7.0',\n",
    "                    'sentence-transformers==2.2.2'],\n",
    "    output_path = \"../tests/package_auto_assembler/\",\n",
    "    prefix = \"requirements_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['numpy',\n",
       " 'dill==0.3.7',\n",
       " 'attrs>=22.2.0',\n",
       " 'requests==2.31.0',\n",
       " 'hnswlib==0.7.0',\n",
       " 'sentence-transformers==2.2.2']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh.read_requirements_file(\n",
    "    # required\n",
    "    requirements_filepath = \"../tests/package_auto_assembler/requirements_mock_vector_database.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing MetadataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MetadataHandler(\n",
    "    # optional/required later\n",
    "    module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if metadata is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh.is_metadata_available(\n",
    "    # optional \n",
    "    module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata from module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Kyrylo Mordan',\n",
       " 'author_email': 'parachute.repo@gmail.com',\n",
       " 'version': '0.0.1',\n",
       " 'description': 'A mock handler for simulating a vector database.',\n",
       " 'keywords': ['python', 'vector database', 'similarity search']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh.get_package_metadata(\n",
    "    # optional \n",
    "    module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Merging local dependacies into single module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing LocalDependaciesHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldh = LocalDependaciesHandler(\n",
    "    # required\n",
    "    main_module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\",\n",
    "    dependencies_dir = \"../tests/package_auto_assembler/dependancies/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine main module with dependacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Mock Vector Db Handler\n",
      "\n",
      "This class is a mock handler for simulating a vector database, designed primarily for testing and development scenarios.\n",
      "It offers functionalities such as text embedding, hierarchical navigable small world (HNSW) search,\n",
      "and basic data management within a simulated environment resembling a vector database.\n",
      "\"\"\"\n",
      "\n",
      "import logging\n",
      "import json\n",
      "import time\n",
      "import numpy as np #==1.26.0\n",
      "import dill #==0.3.7\n",
      "import attr #>=22.2.0\n",
      "import requests #==2.31.0\n",
      "import hnswlib #==0.7.0\n",
      "from sentence_transformers import SentenceTransformer #==2.2.2\n",
      "import sklearn\n",
      "import string\n",
      "import os\n",
      "import csv\n",
      "from collections import Counter\n",
      "from datetime import datetime #==5.2\n",
      "import dill #==5.0.1\n",
      "import pandas as pd #==2.1.1\n",
      "from sklearn.metrics.pairwise import cosine_similarity #==1.3.1\n",
      "\n",
      "@attr.s\n",
      "class Shouter:\n",
      "\n",
      "    \"\"\"\n",
      "    A class for managing and displaying formatted log messages.\n",
      "\n",
      "    This class uses the logging module to create and manage a logger\n",
      "    for displaying formatted messages. It provides a method to output\n",
      "    various types of lines and headers, with customizable message and\n",
      "    line lengths.\n",
      "    \"\"\"\n",
      "\n",
      "    # Formatting settings\n",
      "    dotline_length = attr.ib(default=50)\n",
      "\n",
      "    # Logger settings\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Shouter')\n",
      "    loggerLvl = attr.ib(default=logging.DEBUG)\n",
      "    logger_format = attr.ib(default='(%(asctime)s) : %(name)s : [%(levelname)s] : %(message)s')\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self.initialize_logger()\n",
      "\n",
      "    def initialize_logger(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Initialize a logger for the class instance based on\n",
      "        the specified logging level and logger name.\n",
      "        \"\"\"\n",
      "\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl,format=self.logger_format)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "\n",
      "            self.logger = logger\n",
      "\n",
      "    def shout(self,\n",
      "              mess : str = None,\n",
      "              dotline_length : int = None,\n",
      "              output_type : str = None,\n",
      "              logger : logging.Logger = None) -> None:\n",
      "        \"\"\"\n",
      "        Prints a formatted line or message to the log with various styling options.\n",
      "\n",
      "        This method allows for different types of message formatting in the log, such as\n",
      "        dotted lines, solid lines, or various header styles. The method can be customized\n",
      "        with a specific message, line length, and output type.\n",
      "\n",
      "        Args:\n",
      "            mess (str, optional):\n",
      "                The text message to be included in the log. If not specified, only a line\n",
      "                or header (based on output_type) will be printed. Defaults to None.\n",
      "            dotline_length (int, optional):\n",
      "                The length of the line to be printed. If not specified, the class attribute\n",
      "                `dotline_length` is used. Defaults to None.\n",
      "            output_type (str, optional):\n",
      "                The type of formatting to be applied. Options include \"dline\" for a dotted line,\n",
      "                \"line\" for a solid line, \"pline\" for a period-separated line, \"HEAD1\", \"HEAD2\",\n",
      "                \"HEAD3\" for various header styles, \"title\", \"subtitle\", \"subtitle2\", \"subtitle3\",\n",
      "                and \"warning\" for different emphasis styles. Defaults to \"dline\".\n",
      "            logger (logging.Logger, optional):\n",
      "                A specific logger instance to be used for logging. If not provided, the class's\n",
      "                own logger instance is used. Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "\n",
      "        Examples:\n",
      "            shouter_instance.shout(\"HEAD1\", mess=\"Important Header\", dotline_length=50)\n",
      "            shouter_instance.shout(output_type=\"warning\", mess=\"Caution!\", dotline_length=30)\n",
      "        \"\"\"\n",
      "\n",
      "        if output_type is None:\n",
      "\n",
      "            if mess is not None:\n",
      "                output_type = 'subtitle'\n",
      "            else:\n",
      "                output_type = 'dline'\n",
      "\n",
      "        if dotline_length is None:\n",
      "            dotline_length = self.dotline_length\n",
      "\n",
      "        if logger is None:\n",
      "            logger = self.logger\n",
      "\n",
      "\n",
      "        switch = {\n",
      "            \"dline\": lambda: logger.info(\"=\" * dotline_length),\n",
      "            \"line\": lambda: logger.debug(\"-\" * dotline_length),\n",
      "            \"pline\": lambda: logger.debug(\".\" * dotline_length),\n",
      "            \"HEAD1\": lambda: logger.info(\"\".join([\"\\n\",\n",
      "                                                \"=\" * dotline_length,\n",
      "                                                \"\\n\",\n",
      "                                                \"-\" * ((dotline_length - len(mess)) // 2 - 1),\n",
      "                                                mess,\n",
      "                                                \"-\" * ((dotline_length - len(mess)) // 2 - 1),\n",
      "                                                \" \\n\",\n",
      "                                                \"=\" * dotline_length])),\n",
      "            \"HEAD2\": lambda: logger.info(\"\".join([\"\\n\",\n",
      "                                                \"*\" * ((dotline_length - len(mess)) // 2 - 1),\n",
      "                                                mess,\n",
      "                                                \"*\" * ((dotline_length - len(mess)) // 2 - 1)])),\n",
      "            \"HEAD3\": lambda: logger.info(\"\".join([\"\\n\",\n",
      "                                                \"/\" * ((dotline_length - 10 - len(mess)) // 2 - 1),\n",
      "                                                mess,\n",
      "                                                \"\\\\\" * ((dotline_length - 10 - len(mess)) // 2 - 1)])),\n",
      "            \"title\": lambda: logger.info(f\"** {mess}\"),\n",
      "            \"subtitle\": lambda: logger.info(f\"*** {mess}\"),\n",
      "            \"subtitle2\": lambda: logger.debug(f\"+++ {mess}\"),\n",
      "            \"subtitle3\": lambda: logger.debug(f\"++++ {mess}\"),\n",
      "            \"warning\": lambda: logger.warning(f\"!!! {mess} !!!\"),\n",
      "        }\n",
      "\n",
      "        switch[output_type]()\n",
      "\n",
      "# Metadata for package creation\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class ComparisonFrame:\n",
      "\n",
      "    \"\"\"\n",
      "    Compares query:response pairs expected vs recieved with semantic similarity\n",
      "    and simple metrics of word count, line count etc.\n",
      "\n",
      "    ...\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    embedder : SentenceTransformer\n",
      "        The model used to generate embeddings for semantic comparison.\n",
      "    record_file : str\n",
      "        The name of the CSV file where queries and expected results are stored.\n",
      "    results_file : str\n",
      "        The name of the CSV file where comparison results are stored.\n",
      "    embeddings_file : str\n",
      "        The name of the file where embeddings are stored.\n",
      "    margin_char_count_diff : int\n",
      "        The acceptable margin for character count difference.\n",
      "    margin_word_count_diff : int\n",
      "        The acceptable margin for word count difference.\n",
      "    margin_semantic_similarity : float\n",
      "        The minimum acceptable semantic similarity.\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "    record_query(query, expected_text, overwrite=True)\n",
      "        Records a new query and its expected result in the record file.\n",
      "    mark_query_as_tested(query, test_status)\n",
      "        Marks a query as tested and updates its test status in the record file.\n",
      "    save_embeddings(query, expected_text)\n",
      "        Saves the embeddings for the expected text of a query.\n",
      "    load_embeddings(query)\n",
      "        Loads the saved embeddings for a query.\n",
      "    get_all_queries(untested_only=False)\n",
      "        Retrieves all queries or only untested ones from the record file.\n",
      "    get_comparison_results()\n",
      "        Retrieves the comparison results as a DataFrame.\n",
      "    get_all_records()\n",
      "        Retrieves all records from the record file.\n",
      "    flush_records()\n",
      "        Clears all records from the record file.\n",
      "    flush_comparison_results()\n",
      "        Deletes the comparison results file.\n",
      "    compare_with_record(query, provided_text, mark_as_tested=True)\n",
      "        Compares a provided text with the recorded expected result of a query.\n",
      "    compare(exp_text, prov_text, query='')\n",
      "        Compares two texts and returns the comparison metrics.\n",
      "    compare_char_count(exp_text, prov_text)\n",
      "        Computes the difference in character count between two texts.\n",
      "    compare_word_count(exp_text, prov_text)\n",
      "        Computes the difference in word count between two texts.\n",
      "    compare_line_count(exp_text, prov_text)\n",
      "        Computes the difference in line count between two texts.\n",
      "    compare_punctuation(exp_text, prov_text)\n",
      "        Computes the difference in punctuation usage between two texts.\n",
      "    compare_semantic_similarity(exp_text, prov_text)\n",
      "        Computes the semantic similarity between two texts.\n",
      "    reset_record_statuses(record_ids=None)\n",
      "        Resets the 'tested' status of specific queries or all queries in the record file, making them available for re-testing. Accepts an optional list of record IDs to reset; otherwise, resets all records.\n",
      "    \"\"\"\n",
      "\n",
      "    embedder = attr.ib(default=None)\n",
      "    model_name = attr.ib(default='all-mpnet-base-v2')\n",
      "\n",
      "    # Files saved to persist\n",
      "    record_file = attr.ib(default=\"record_file.csv\")  # file where queries and expected results are stored\n",
      "    results_file = attr.ib(default=\"comparison_results.csv\") # file where comparison results will be stored\n",
      "    embeddings_file = attr.ib(default=\"embeddings.dill\")\n",
      "\n",
      "    # Define acceptable margins\n",
      "    margin_char_count_diff = attr.ib(default=10)\n",
      "    margin_word_count_diff = attr.ib(default=5)\n",
      "    margin_semantic_similarity = attr.ib(default=0.95)\n",
      "\n",
      "    # Logger settings\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='ComparisonFrame')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self.initialize_logger()\n",
      "        self.initialize_record_file()\n",
      "        self.initialize_embedder()\n",
      "\n",
      "\n",
      "    def initialize_logger(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Initialize a logger for the class instance based on\n",
      "        the specified logging level and logger name.\n",
      "        \"\"\"\n",
      "\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "\n",
      "            self.logger = logger\n",
      "\n",
      "    def initialize_record_file(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Initialize empty records file and saves it locally\n",
      "        if it was not found in specified location.\n",
      "        \"\"\"\n",
      "\n",
      "        # Create a new file with headers if it doesn't exist\n",
      "        if not os.path.isfile(self.record_file):\n",
      "            with open(self.record_file, mode='w', newline='', encoding='utf-8') as file:\n",
      "                writer = csv.writer(file)\n",
      "                # Include 'test_status' in the headers from the beginning\n",
      "                writer.writerow(['id', 'timestamp', 'query', 'expected_text', 'tested', 'test_status'])  # Added 'test_status'\n",
      "\n",
      "\n",
      "    def initialize_embedder(self, model_name : str = None, reset : bool = True):\n",
      "\n",
      "        \"\"\"\n",
      "        Initialize embedder for the class instance for a chosen model from sentence_transformer.\n",
      "        \"\"\"\n",
      "\n",
      "        if model_name is None:\n",
      "            model_name = self.model_name\n",
      "\n",
      "        if (self.embedder is None) or reset:\n",
      "\n",
      "            if model_name:\n",
      "                try:\n",
      "                    self.embedder = SentenceTransformer(model_name)\n",
      "                    self.model_name = model_name\n",
      "                except Exception as e:\n",
      "                    self.logger.error(\"Provided model name was not loaded!\")\n",
      "                    print(e)\n",
      "\n",
      "            else:\n",
      "                self.logger.error(\"Model name is missing!\")\n",
      "                self.logger.error(\"Either provide 'embedder' parameter or 'model_name' parameter!\")\n",
      "                raise ValueError(\"Missing 'model_name' parameter!\")\n",
      "        else:\n",
      "\n",
      "            # check if embedder is from sentence transformer\n",
      "            if not isinstance(embedder, SentenceTransformer):\n",
      "                self.logger.warning(\"Provided embedder should be from sentence_transformer package!\")\n",
      "                raise TypeError(\"embedder is not an instance of SentenceTransformer\")\n",
      "\n",
      "\n",
      "    def record_query(self, query, expected_text, overwrite=True):\n",
      "\n",
      "        \"\"\"\n",
      "        Records a new query and its expected result in the record file.\n",
      "        \"\"\"\n",
      "\n",
      "        rows = []\n",
      "        max_id = 0\n",
      "        # Read the existing data\n",
      "        if os.path.isfile(self.record_file):\n",
      "            with open(self.record_file, mode='r', encoding='utf-8') as file:\n",
      "                reader = csv.reader(file)\n",
      "                rows = list(reader)\n",
      "                if len(rows) > 1:  # if there's more than just the header\n",
      "                    # find the maximum id (which is in the first column and convert it to int)\n",
      "                    max_id = max(int(row[0]) for row in rows[1:])\n",
      "\n",
      "        new_id = max_id + 1\n",
      "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # current time in string format\n",
      "\n",
      "        # If overwrite is True, update the existing record if the query exists; otherwise, append the new record\n",
      "        if overwrite:\n",
      "            for index, row in enumerate(rows):\n",
      "                if len(row) > 2 and row[2] == query:  # queries are in the third column\n",
      "                    rows[index] = [str(new_id), current_time, query, expected_text, 'no', '']  # 'no' indicates untested, '' for empty test_status\n",
      "                    break\n",
      "            else:\n",
      "                rows.append([str(new_id), current_time, query, expected_text, 'no', ''])  # 'no' indicates untested, '' for empty test_status\n",
      "        else:\n",
      "            rows.append([str(new_id), current_time, query, expected_text, 'no', ''])  # 'no' indicates untested, '' for empty test_status\n",
      "\n",
      "        self.save_embeddings(query=query,\n",
      "                             expected_text=expected_text)\n",
      "\n",
      "        # Write the updated data back to the file\n",
      "        with open(self.record_file, mode='w', newline='', encoding='utf-8') as file:\n",
      "            writer = csv.writer(file)\n",
      "            writer.writerows(rows)\n",
      "\n",
      "    def mark_query_as_tested(self, query, test_status):\n",
      "        \"\"\"\n",
      "        Updates the 'tested' status and 'test_status' of a specific query in the record file.\n",
      "        \"\"\"\n",
      "\n",
      "        # Read the existing data\n",
      "        rows = []\n",
      "        with open(self.record_file, mode='r', encoding='utf-8') as file:\n",
      "            reader = csv.reader(file)\n",
      "            rows = list(reader)\n",
      "\n",
      "        headers = rows[0]  # Extract the headers\n",
      "        # Check if 'test_status' is in headers, if not, add it\n",
      "        if 'test_status' not in headers:\n",
      "            headers.append('test_status')\n",
      "\n",
      "        # Find the query and mark it as tested, and update the test status\n",
      "        for row in rows[1:]:  # Skip the header row\n",
      "            if row[2] == query:  # if the query matches\n",
      "                row[4] = 'yes'  # 'yes' indicates tested\n",
      "                if len(row) >= 6:  # if 'test_status' column exists\n",
      "                    row[5] = test_status  # update the 'test_status' column\n",
      "                else:\n",
      "                    row.append(test_status)  # if 'test_status' column doesn't exist, append the status\n",
      "\n",
      "        # Write the updated data back to the file, including the headers\n",
      "        with open(self.record_file, mode='w', newline='', encoding='utf-8') as file:\n",
      "            writer = csv.writer(file)\n",
      "            writer.writerow(headers)  # Write the headers first\n",
      "            writer.writerows(rows[1:])  # Write the data rows\n",
      "\n",
      "\n",
      "    def reset_record_statuses(self, record_ids=None):\n",
      "        \"\"\"\n",
      "        Resets the 'tested' status of specific queries or all queries in the record file, making them available for re-testing.\n",
      "\n",
      "        Parameters:\n",
      "        record_ids (list of int): Optional. A list of record IDs for which to reset the statuses. If None, all records are reset.\n",
      "        \"\"\"\n",
      "\n",
      "        # Read the existing data\n",
      "        with open(self.record_file, mode='r', encoding='utf-8') as file:\n",
      "            reader = csv.reader(file)\n",
      "            rows = list(reader)\n",
      "\n",
      "        # Check for the right headers and adjust the data rows\n",
      "        headers = rows[0]  # Extract the headers\n",
      "        if 'test_status' not in headers:\n",
      "            headers.append('test_status')  # Add 'test_status' to headers if it's missing\n",
      "\n",
      "        new_rows = [headers]  # Include the headers as the first row\n",
      "\n",
      "        for row in rows[1:]:  # Skip the header row\n",
      "            if record_ids is None or int(row[0]) in record_ids:  # Check if resetting all or specific IDs\n",
      "                new_row = row[:5]  # Select columns 'id' through 'tested'\n",
      "                new_row[4] = 'no'  # 'no' indicates untested\n",
      "                if len(row) == 6:  # if 'test_status' column exists\n",
      "                    new_row.append('')  # reset 'test_status' to an empty string\n",
      "                else:\n",
      "                    new_row.append('')  # if 'test_status' column doesn't exist, still add an empty string placeholder\n",
      "                new_rows.append(new_row)\n",
      "            else:\n",
      "                new_rows.append(row)  # If the ID is not in the list, keep the row unchanged\n",
      "\n",
      "        # Write the updated data back to the file, including the headers\n",
      "        with open(self.record_file, mode='w', newline='', encoding='utf-8') as file:\n",
      "            writer = csv.writer(file)\n",
      "            writer.writerows(new_rows)  # Write the updated rows back to CSV, including headers\n",
      "\n",
      "    def save_embeddings(self, query, expected_text):\n",
      "\n",
      "        \"\"\"\n",
      "        Generates and stores the embeddings for the expected text of a specific query.\n",
      "        \"\"\"\n",
      "\n",
      "        # Generate embeddings\n",
      "        embeddings = self.embedder.encode(expected_text)\n",
      "\n",
      "        # Load existing data or create new\n",
      "        if os.path.exists(self.embeddings_file):\n",
      "            with open(self.embeddings_file, 'rb') as file:\n",
      "                data = dill.load(file)\n",
      "        else:\n",
      "            data = {}\n",
      "\n",
      "        # Add new embeddings\n",
      "        data[query] = embeddings\n",
      "\n",
      "        # Save data\n",
      "\n",
      "        embeddings_with_model_name = {'model_name' : self.model_name,\n",
      "                                      'embeddings' : data}\n",
      "\n",
      "        with open(self.embeddings_file, 'wb') as file:\n",
      "            dill.dump(embeddings_with_model_name, file)\n",
      "\n",
      "    def load_embeddings(self, query):\n",
      "\n",
      "        \"\"\"\n",
      "        Retrieves the stored embeddings for a specific query.\n",
      "        \"\"\"\n",
      "\n",
      "        # Check if embeddings file exists\n",
      "        if not os.path.exists(self.embeddings_file):\n",
      "            raise FileNotFoundError(\"No embeddings file found. Please generate embeddings first.\")\n",
      "\n",
      "        # Load data\n",
      "        with open(self.embeddings_file, 'rb') as file:\n",
      "            embeddings_with_model_name = dill.load(file)\n",
      "\n",
      "        self.logger.debug(f\"Model name for the loaded embeddings: {embeddings_with_model_name['model_name']}\")\n",
      "\n",
      "        # Retrieve embeddings for the given query\n",
      "        embeddings = embeddings_with_model_name['embeddings'].get(query)\n",
      "\n",
      "        if embeddings is None:\n",
      "            raise ValueError(f\"No embeddings found for query: {query}\")\n",
      "\n",
      "        return embeddings\n",
      "\n",
      "\n",
      "    def get_all_queries(self, untested_only=False):\n",
      "\n",
      "        \"\"\"\n",
      "        Retrieves a list of all recorded queries, with an option to return only those that haven't been tested.\n",
      "        \"\"\"\n",
      "\n",
      "        # Read the recorded data and retrieve all queries\n",
      "        queries = []\n",
      "        with open(self.record_file, mode='r', encoding='utf-8') as file:\n",
      "            reader = csv.reader(file)\n",
      "            next(reader)  # skip headers\n",
      "            if untested_only:\n",
      "                queries = [row[2] for row in reader if row[4] == 'no']  # select only untested queries\n",
      "            else:\n",
      "                queries = [row[2] for row in reader]  # select all queries\n",
      "\n",
      "        return queries\n",
      "\n",
      "    def get_comparison_results(self, throw_error : bool = False):\n",
      "\n",
      "        \"\"\"\n",
      "        Retrieves the comparison results as a DataFrame from the stored file.\n",
      "        \"\"\"\n",
      "\n",
      "        # Check if the results file exists\n",
      "        if not os.path.isfile(self.results_file):\n",
      "            error_mess = \"No results file found. Please perform some comparisons first.\"\n",
      "            if throw_error:\n",
      "                raise FileNotFoundError(error_mess)\n",
      "            else:\n",
      "                self.logger.error(error_mess)\n",
      "\n",
      "        else:\n",
      "            # Read the CSV file into a pandas DataFrame\n",
      "            df = pd.read_csv(self.results_file)\n",
      "\n",
      "            return df\n",
      "\n",
      "    def get_all_records(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Retrieves all query records as a DataFrame from the stored file.\n",
      "        \"\"\"\n",
      "\n",
      "        # Check if the record file exists\n",
      "        if not os.path.isfile(self.record_file):\n",
      "            raise FileNotFoundError(\"No record file found. Please record some queries first.\")\n",
      "\n",
      "        # Read the CSV file into a pandas DataFrame\n",
      "        df = pd.read_csv(self.record_file)\n",
      "\n",
      "        return df\n",
      "\n",
      "    def flush_records(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Clears all query records from the stored file, leaving only the headers.\n",
      "        \"\"\"\n",
      "\n",
      "        # Open the file in write mode to clear it, then write back only the headers\n",
      "        with open(self.record_file, mode='w', newline='', encoding='utf-8') as file:\n",
      "            writer = csv.writer(file)\n",
      "            writer.writerow(['id', 'timestamp', 'query', 'expected_text', 'tested', 'test_status'])  # column headers\n",
      "\n",
      "    def flush_comparison_results(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Deletes the file containing the comparison results.\n",
      "        \"\"\"\n",
      "\n",
      "        # Check if the results file exists\n",
      "        if os.path.isfile(self.results_file):\n",
      "            os.remove(self.results_file)\n",
      "        else:\n",
      "            raise FileNotFoundError(\"No results file found. There's nothing to flush.\")\n",
      "\n",
      "    def compare_with_record(self,\n",
      "                            query : str,\n",
      "                            provided_text : str,\n",
      "                            mark_as_tested : bool = True,\n",
      "                            return_results : bool = False):\n",
      "\n",
      "        \"\"\"\n",
      "        Compares the provided text with all recorded expected results for a specific query and stores the comparison results.\n",
      "        \"\"\"\n",
      "\n",
      "        # Read the recorded data and find all records for the query, sorted by timestamp\n",
      "        with open(self.record_file, mode='r', encoding='utf-8') as file:\n",
      "            reader = csv.reader(file)\n",
      "            # Skip the header, find all rows with the matching query, and sort them by the timestamp\n",
      "            records = sorted(\n",
      "                (row for row in reader if len(row) > 2 and row[2] == query),\n",
      "                key=lambda x: x[1],\n",
      "                reverse=True  # most recent first\n",
      "            )\n",
      "\n",
      "        if not records:\n",
      "            raise ValueError(\"Query not found in records.\")\n",
      "\n",
      "        comparisons = []\n",
      "        for record in records:\n",
      "            expected_text = record[3]  # expected text is in the fourth column\n",
      "            comparison = self.compare(expected_text, provided_text, query=query)\n",
      "            comparison['id'] = record[0]  # id is in the first column\n",
      "            comparisons.append(comparison)\n",
      "\n",
      "\n",
      "        # After conducting the comparison\n",
      "        for comparison in comparisons:\n",
      "            # Check if differences are within acceptable margins\n",
      "            passed_char_count = comparison['char_count_diff'] <= self.margin_char_count_diff\n",
      "            passed_word_count = comparison['word_count_diff'] <= self.margin_word_count_diff\n",
      "            passed_semantic_similarity = comparison['semantic_similarity'] >= self.margin_semantic_similarity\n",
      "\n",
      "            # If all checks pass, mark as 'pass'; otherwise, 'fail'\n",
      "            if passed_char_count and passed_word_count and passed_semantic_similarity:\n",
      "                test_status = 'pass'\n",
      "            else:\n",
      "                test_status = 'fail'\n",
      "\n",
      "            # If required, mark the query as tested with the test status\n",
      "            if mark_as_tested:\n",
      "                self.mark_query_as_tested(query, test_status)\n",
      "\n",
      "        # Convert results list to DataFrame\n",
      "        results_df = pd.DataFrame(comparisons)\n",
      "\n",
      "        # Save results DataFrame to CSV\n",
      "        # 'mode='a'' will append the results to the existing file;\n",
      "        # 'header=not os.path.isfile(self.results_file)' will write headers only if the file doesn't already exist\n",
      "        results_df.to_csv(self.results_file, mode='a', header=not os.path.isfile(self.results_file), index=False)\n",
      "\n",
      "        if return_results:\n",
      "            return results_df\n",
      "\n",
      "    def compare(self, exp_text : str, prov_text : str, query : str = ''):\n",
      "\n",
      "        \"\"\"\n",
      "        Performs a detailed comparison between two texts, providing metrics like character count, word count, semantic similarity, etc.\n",
      "        \"\"\"\n",
      "\n",
      "        results = {\n",
      "            'query' : query,\n",
      "            'char_count_diff': self.compare_char_count(exp_text, prov_text),\n",
      "            'word_count_diff': self.compare_word_count(exp_text, prov_text),\n",
      "            'line_count_diff': self.compare_line_count(exp_text, prov_text),\n",
      "            'punctuation_diff': self.compare_punctuation(exp_text, prov_text),\n",
      "            'semantic_similarity': self.compare_semantic_similarity(exp_text, prov_text),\n",
      "            'expected_text' : exp_text,\n",
      "            'provided_text' : prov_text\n",
      "        }\n",
      "\n",
      "        return results\n",
      "\n",
      "    def compare_char_count(self, exp_text, prov_text):\n",
      "\n",
      "        \"\"\"\n",
      "        Calculates the absolute difference in the number of characters between two texts.\n",
      "        \"\"\"\n",
      "\n",
      "        return abs(len(exp_text) - len(prov_text))\n",
      "\n",
      "    def compare_word_count(self, exp_text, prov_text):\n",
      "\n",
      "        \"\"\"\n",
      "        Calculates the absolute difference in the number of words between two texts.\n",
      "        \"\"\"\n",
      "\n",
      "        return abs(len(exp_text.split()) - len(prov_text.split()))\n",
      "\n",
      "    def compare_line_count(self, exp_text, prov_text):\n",
      "\n",
      "        \"\"\"\n",
      "        Calculates the absolute difference in the number of lines between two texts.\n",
      "        \"\"\"\n",
      "\n",
      "        return abs(len(exp_text.splitlines()) - len(prov_text.splitlines()))\n",
      "\n",
      "    def compare_punctuation(self, exp_text, prov_text):\n",
      "\n",
      "        \"\"\"\n",
      "        Calculates the total difference in the use of punctuation characters between two texts.\n",
      "        \"\"\"\n",
      "\n",
      "        punctuation1 = Counter(char for char in exp_text if char in string.punctuation)\n",
      "        punctuation2 = Counter(char for char in prov_text if char in string.punctuation)\n",
      "        return sum((punctuation1 - punctuation2).values()) + sum((punctuation2 - punctuation1).values())\n",
      "\n",
      "\n",
      "    def compare_semantic_similarity(self, exp_text, prov_text):\n",
      "\n",
      "        \"\"\"\n",
      "        Computes the semantic similarity between two pieces of text using their embeddings.\n",
      "        \"\"\"\n",
      "\n",
      "        embedding1 = self.embedder.encode(exp_text).reshape(1, -1)\n",
      "        embedding2 = self.embedder.encode(prov_text).reshape(1, -1)\n",
      "        return cosine_similarity(embedding1, embedding2)[0][0]\n",
      "\n",
      "\"\"\"\n",
      "Mock Vector Db Handler\n",
      "\n",
      "This class is a mock handler for simulating a vector database, designed primarily for testing and development scenarios.\n",
      "It offers functionalities such as text embedding, hierarchical navigable small world (HNSW) search,\n",
      "and basic data management within a simulated environment resembling a vector database.\n",
      "\"\"\"\n",
      "\n",
      "# Imports\n",
      "## essential\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## for search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Metadata for package creation\n",
      "__package_metadata__ = {\n",
      "    \"author\": \"Kyrylo Mordan\",\n",
      "    \"author_email\": \"parachute.repo@gmail.com\",\n",
      "    \"version\": \"0.0.1\",\n",
      "    \"description\": \"A mock handler for simulating a vector database.\",\n",
      "    \"keywords\" : ['python', 'vector database', 'similarity search']\n",
      "    # Add other metadata as needed\n",
      "}\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockVecDbHandler:\n",
      "    # pylint: disable=too-many-instance-attributes\n",
      "\n",
      "    \"\"\"\n",
      "    The MockVecDbHandler class simulates a vector database environment, primarily for testing and development purposes.\n",
      "    It integrates various functionalities such as text embedding, Hierarchical Navigable Small World (HNSW) search,\n",
      "    and basic data management, mimicking operations in a real vector database.\n",
      "\n",
      "    Parameters:\n",
      "        embeddings_url (str): URL to access OpenAI models for generating embeddings, crucial for text analysis.\n",
      "        godID (str): Unique identifier for authentication with the embedding service.\n",
      "        headers (dict): HTTP headers for API interactions with the embedding service.\n",
      "        file_path (str): Local file path for storing and simulating the database; defaults to \"../redis_mock\".\n",
      "        persist (bool): Flag to persist data changes; defaults to False.\n",
      "        embedder_error_tolerance (float): Tolerance level for embedding errors; defaults to 0.0.\n",
      "        logger (logging.Logger): Logger instance for activity logging.\n",
      "        logger_name (str): Name for the logger; defaults to 'Mock handler'.\n",
      "        loggerLvl (int): Logging level, set to logging.INFO by default.\n",
      "        return_keys_list (list): Fields to return in search results; defaults to an empty list.\n",
      "        search_results_n (int): Number of results to return in searches; defaults to 3.\n",
      "        similarity_search_type (str): Type of similarity search to use; defaults to 'hnsw'.\n",
      "        similarity_params (dict): Parameters for similarity search; defaults to {'space':'cosine'}.\n",
      "\n",
      "    Attributes:\n",
      "        data (dict): In-memory representation of database contents.\n",
      "        filtered_data (dict): Stores filtered database entries based on criteria.\n",
      "        keys_list (list): List of keys in the database.\n",
      "        results_keys (list): Keys matching specific search criteria.\n",
      "\n",
      "    Methods:\n",
      "        initialize_logger()\n",
      "            Sets up logging for the class instance.\n",
      "\n",
      "        hnsw_search(search_emb, doc_embs, k=1, space='cosine', ef_search=50, M=16, ef_construction=200)\n",
      "            Performs HNSW algorithm-based search.\n",
      "\n",
      "        linear_search(search_emb, doc_embs, k=1, space='cosine')\n",
      "            Conducts a linear search.\n",
      "\n",
      "        establish_connection(file_path=None)\n",
      "            Simulates establishing a database connection.\n",
      "\n",
      "        save_data()\n",
      "            Saves the current state of the 'data' attribute to a file.\n",
      "\n",
      "        embed(text)\n",
      "            Generates embeddings for text inputs.\n",
      "\n",
      "        _prepare_for_redis(data_dict, var_for_embedding_name)\n",
      "            Prepares data for storage in Redis.\n",
      "\n",
      "        insert_values_dict(values_dict, var_for_embedding_name)\n",
      "            Simulates insertion of key-value pairs into the database.\n",
      "\n",
      "        flush_database()\n",
      "            Clears all data in the mock database.\n",
      "\n",
      "        filter_keys(subkey=None, subvalue=None)\n",
      "            Filters data entries based on a specific subkey and subvalue.\n",
      "\n",
      "        filter_database(filter_criteria=None)\n",
      "            Filters a dictionary based on multiple field criteria.\n",
      "\n",
      "        remove_from_database(filter_criteria=None)\n",
      "            Removes key-value pairs from a dictionary based on filter criteria.\n",
      "\n",
      "        search_database_keys(query, search_results_n=None, similarity_search_type=None, similarity_params=None)\n",
      "            Searches the database using embeddings and saves a list of entries that match the query.\n",
      "\n",
      "        get_dict_results(return_keys_list=None)\n",
      "            Retrieves specified fields from the search results.\n",
      "\n",
      "        search_database(query, search_results_n=None, filter_criteria=None, similarity_search_type=None,\n",
      "                        similarity_params=None, return_keys_list=None)\n",
      "            Searches and retrieves fields from the database for a given filter.\n",
      "    \"\"\"\n",
      "\n",
      "    ## for accessing openAI models\n",
      "    embeddings_url = attr.ib(default=None)\n",
      "    godID = attr.ib(default=None)\n",
      "    headers = attr.ib(default=None)\n",
      "\n",
      "    ## for embeddings\n",
      "    model_type = attr.ib(default='sentence_transformer', type=str)\n",
      "    st_model_name = attr.ib(default='all-MiniLM-L6-v2', type=str)\n",
      "    st_model = attr.ib(default=None, init=False)\n",
      "\n",
      "\n",
      "    ## for similarity search\n",
      "    return_keys_list = attr.ib(default=[], type = list)\n",
      "    search_results_n = attr.ib(default=3, type = int)\n",
      "    similarity_search_type = attr.ib(default='linear', type = str)\n",
      "    similarity_params = attr.ib(default={'space':'cosine'}, type = dict)\n",
      "\n",
      "    ## inputs with defaults\n",
      "    file_path = attr.ib(default=\"../redis_mock\", type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "\n",
      "    ## outputs\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init = False)\n",
      "    results_keys = attr.ib(default=None, init = False)\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self._initialize_logger()\n",
      "        if self.model_type != 'openAI':\n",
      "            self.st_model = SentenceTransformer(self.st_model_name)\n",
      "\n",
      "    def _initialize_logger(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Initialize a logger for the class instance based on the specified logging level and logger name.\n",
      "        \"\"\"\n",
      "\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "\n",
      "            self.logger = logger\n",
      "\n",
      "    def hnsw_search(self, search_emb, doc_embs, k=1, space='cosine', ef_search=50, M=16, ef_construction=200):\n",
      "        \"\"\"\n",
      "        Perform Hierarchical Navigable Small World search.\n",
      "\n",
      "        Args:\n",
      "        - search_emb (numpy array): The query embedding. Shape (1, dim).\n",
      "        - doc_embs (numpy array): Array of reference embeddings. Shape (num_elements, dim).\n",
      "        - k (int): Number of nearest neighbors to return.\n",
      "        - space (str): Space type for the index ('cosine' or 'l2').\n",
      "        - ef_search (int): Search parameter. Higher means more accurate but slower.\n",
      "        - M (int): Index parameter.\n",
      "        - ef_construction (int): Index construction parameter.\n",
      "\n",
      "        Returns:\n",
      "        - labels (numpy array): Indices of the k nearest embeddings from doc_embs to search_emb.\n",
      "        - distances (numpy array): Distances of the k nearest embeddings.\n",
      "        \"\"\"\n",
      "\n",
      "        # Declare index\n",
      "        dim = len(search_emb)#.shape[1]\n",
      "        p = hnswlib.Index(space=space, dim=dim)\n",
      "\n",
      "        # Initialize the index using the data\n",
      "        p.init_index(max_elements=len(doc_embs), ef_construction=ef_construction, M=M)\n",
      "\n",
      "        # Add data to index\n",
      "        p.add_items(doc_embs)\n",
      "\n",
      "        # Set the query ef parameter\n",
      "        p.set_ef(ef_search)\n",
      "\n",
      "        # Query the index\n",
      "        labels, distances = p.knn_query(search_emb, k=k)\n",
      "\n",
      "        return labels[0], distances[0]\n",
      "\n",
      "    def linear_search(self, search_emb, doc_embs, k=1, space='cosine'):\n",
      "\n",
      "        \"\"\"\n",
      "        Perform a linear (brute force) search.\n",
      "\n",
      "        Args:\n",
      "        - search_emb (numpy array): The query embedding. Shape (1, dim).\n",
      "        - doc_embs (numpy array): Array of reference embeddings. Shape (num_elements, dim).\n",
      "        - k (int): Number of nearest neighbors to return.\n",
      "        - space (str): Space type for the distance calculation ('cosine' or 'l2').\n",
      "\n",
      "        Returns:\n",
      "        - labels (numpy array): Indices of the k nearest embeddings from doc_embs to search_emb.\n",
      "        - distances (numpy array): Distances of the k nearest embeddings.\n",
      "        \"\"\"\n",
      "\n",
      "        # Calculate distances from the query to all document embeddings\n",
      "        if space == 'cosine':\n",
      "            # Normalize embeddings for cosine similarity\n",
      "            search_emb_norm = search_emb / np.linalg.norm(search_emb)\n",
      "            doc_embs_norm = doc_embs / np.linalg.norm(doc_embs, axis=1)[:, np.newaxis]\n",
      "\n",
      "            # Compute cosine distances\n",
      "            distances = np.dot(doc_embs_norm, search_emb_norm.T).flatten()\n",
      "        elif space == 'l2':\n",
      "            # Compute L2 distances\n",
      "            distances = np.linalg.norm(doc_embs - search_emb, axis=1)\n",
      "\n",
      "        # Get the indices of the top k closest embeddings\n",
      "        if space == 'cosine':\n",
      "            # For cosine, larger values mean closer distance\n",
      "            labels = np.argsort(-distances)[:k]\n",
      "        else:\n",
      "            # For L2, smaller values mean closer distance\n",
      "            labels = np.argsort(distances)[:k]\n",
      "\n",
      "        # Get the distances of the top k closest embeddings\n",
      "        top_distances = distances[labels]\n",
      "\n",
      "        return labels, top_distances\n",
      "\n",
      "    def establish_connection(self, file_path : str = None):\n",
      "\n",
      "        \"\"\"\n",
      "        Simulates establishing a connection by loading data from a local file into the 'data' attribute.\n",
      "        \"\"\"\n",
      "\n",
      "        if file_path is None:\n",
      "            file_path = self.file_path\n",
      "\n",
      "        try:\n",
      "            with open(file_path, 'rb') as file:\n",
      "                self.data = dill.load(file)\n",
      "        except FileNotFoundError:\n",
      "            self.data = {}\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Error loading data from file: \", e)\n",
      "\n",
      "    def save_data(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Saves the current state of 'data' back into a local file.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            with open(self.file_path, 'wb') as file:\n",
      "                dill.dump(self.data, file)\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Error saving data to file: \", e)\n",
      "\n",
      "    def embed(self, text, model_type : str =  None ):\n",
      "\n",
      "        \"\"\"\n",
      "        Embeds single query with sentence with selected embedder.\n",
      "        \"\"\"\n",
      "\n",
      "        if model_type is None:\n",
      "            model_type = self.model_type\n",
      "\n",
      "        if model_type == 'openAI':\n",
      "            return self.embed_openAI(text = text)\n",
      "\n",
      "        if model_type == 'sentence_transformer':\n",
      "            return self.embed_sentence_transformer(text = text)\n",
      "\n",
      "\n",
      "    def embed_sentence_transformer(self, text):\n",
      "\n",
      "        \"\"\"\n",
      "        Embeds single query with sentence tranformer embedder.\n",
      "        \"\"\"\n",
      "\n",
      "        return self.st_model.encode(text)\n",
      "\n",
      "    def embed_openAI(self, text):\n",
      "\n",
      "        \"\"\"\n",
      "        Embeds single query with openAI embedder.\n",
      "        \"\"\"\n",
      "\n",
      "        api_url = self.embeddings_url\n",
      "\n",
      "        payload = json.dumps({\n",
      "            \"user\": self.godID,\n",
      "            \"input\": text\n",
      "        })\n",
      "\n",
      "        try:\n",
      "            response = requests.post(api_url, headers=self.headers, data=payload, timeout=10)\n",
      "\n",
      "            if response.status_code == 429:\n",
      "                time.sleep(1)\n",
      "                response = requests.post(api_url, headers=self.headers, data=payload, timeout=10)\n",
      "\n",
      "            if response.status_code > 200:\n",
      "                print(f\"Request to '{api_url}' failed: {response}\")\n",
      "                print(response.text)\n",
      "                return None\n",
      "\n",
      "            embedding = response.json()['data'][0]['embedding']\n",
      "\n",
      "        except:\n",
      "            error_mess = \"An exception has occurred during embedding!\"\n",
      "            if self.embedder_error_tolerance == 0.0:\n",
      "                raise ValueError(error_mess)\n",
      "            else:\n",
      "                print(error_mess)\n",
      "                return None\n",
      "\n",
      "        return embedding\n",
      "\n",
      "    def _prepare_for_redis(self, data_dict, var_for_embedding_name):\n",
      "\n",
      "        \"\"\"\n",
      "        Prepare a dictionary for storage in Redis by serializing all its values to strings.\n",
      "        \"\"\"\n",
      "\n",
      "        for key, _ in data_dict.items():\n",
      "\n",
      "            embedding = self.embed(data_dict[key][var_for_embedding_name])\n",
      "            data_dict[key]['embedding'] = embedding\n",
      "\n",
      "        return data_dict\n",
      "\n",
      "\n",
      "    def insert_values_dict(self, values_dict, var_for_embedding_name):\n",
      "\n",
      "        \"\"\"\n",
      "        Simulates inserting key-value pairs into the mock Redis database.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "\n",
      "            values_dict = self._prepare_for_redis(data_dict = values_dict,\n",
      "                                                  var_for_embedding_name = var_for_embedding_name)\n",
      "\n",
      "            self.data.update(values_dict)\n",
      "            self.save_data()\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Problem during inserting list of key-values dictionaries into mock database!\", e)\n",
      "\n",
      "    def flush_database(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Clears all data in the mock database.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            self.data = {}\n",
      "            if self.persist:\n",
      "                self.save_data()\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Problem during flushing mock database\", e)\n",
      "\n",
      "    def filter_keys(self, subkey=None, subvalue=None):\n",
      "\n",
      "        \"\"\"\n",
      "        Filters data entries based on a specific subkey and subvalue.\n",
      "        \"\"\"\n",
      "\n",
      "        if (subkey is not None) and (subvalue is not None):\n",
      "            self.keys_list = [d for d in self.data if self.data[d][subkey] == subvalue]\n",
      "        else:\n",
      "            self.keys_list = self.data\n",
      "\n",
      "    def filter_database(self, filter_criteria : dict = None):\n",
      "\n",
      "        \"\"\"\n",
      "        Filters a dictionary based on multiple field criteria.\n",
      "        \"\"\"\n",
      "\n",
      "        self.filtered_data = {\n",
      "            key: value for key, value in self.data.items()\n",
      "            if all(value.get(k) == v for k, v in filter_criteria.items())\n",
      "        }\n",
      "\n",
      "    def remove_from_database(self, filter_criteria : dict = None):\n",
      "        \"\"\"\n",
      "        Removes key-value pairs from a dictionary based on filter criteria.\n",
      "        \"\"\"\n",
      "\n",
      "        self.data = {\n",
      "            key: value for key, value in self.data.items()\n",
      "            if not all(value.get(k) == v for k, v in filter_criteria.items())\n",
      "        }\n",
      "\n",
      "    def search_database_keys(self,\n",
      "        query: str,\n",
      "        search_results_n: int = None,\n",
      "        similarity_search_type: str = None,\n",
      "        similarity_params: dict = None):\n",
      "\n",
      "        \"\"\"\n",
      "        Searches the mock database using embeddings and saves a list of entries that match the query.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            query_embedding = self.embed(query)\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Problem during embedding search query!\", e)\n",
      "\n",
      "\n",
      "        if search_results_n is None:\n",
      "            search_results_n = self.search_results_n\n",
      "\n",
      "        if similarity_search_type is None:\n",
      "            similarity_search_type = self.similarity_search_type\n",
      "\n",
      "        if similarity_params is None:\n",
      "            similarity_params = self.similarity_params\n",
      "\n",
      "        if self.filtered_data is None:\n",
      "            self.filtered_data = self.data\n",
      "\n",
      "        if self.keys_list is None:\n",
      "            self.keys_list = [key for key in self.filtered_data]\n",
      "\n",
      "        try:\n",
      "            data_embeddings = np.array([(self.filtered_data[d]['embedding']) for d in self.keys_list])\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Problem during extracting search pool embeddings!\", e)\n",
      "\n",
      "        try:\n",
      "            if similarity_search_type == 'linear':\n",
      "                labels, _ = self.linear_search(query_embedding,\n",
      "                data_embeddings,\n",
      "                k=search_results_n,\n",
      "                **similarity_params)\n",
      "            else:\n",
      "                labels, _ = self.hnsw_search(query_embedding,\n",
      "                data_embeddings,\n",
      "                k=search_results_n,\n",
      "                **similarity_params)\n",
      "\n",
      "            self.results_keys = [self.keys_list[i] for i in labels]\n",
      "        except Exception as e:\n",
      "            self.logger.error(\"Problem during extracting results from the mock database!\", e)\n",
      "\n",
      "    def get_dict_results(self, return_keys_list : list = None) -> list:\n",
      "\n",
      "        \"\"\"\n",
      "        Retrieves specified fields from the search results in the mock database.\n",
      "        \"\"\"\n",
      "\n",
      "        if return_keys_list is None:\n",
      "            return_keys_list = self.return_keys_list\n",
      "\n",
      "        # This method mimics the behavior of the original 'get_dict_results' method\n",
      "        results = []\n",
      "        for searched_doc in self.results_keys:\n",
      "            result = {key: self.data[searched_doc].get(key) for key in return_keys_list}\n",
      "            results.append(result)\n",
      "        return results\n",
      "\n",
      "    def search_database(self,\n",
      "        query: str,\n",
      "        search_results_n: int = None,\n",
      "        filter_criteria : dict = None,\n",
      "        similarity_search_type: str = None,\n",
      "        similarity_params: dict = None,\n",
      "        return_keys_list : list = None) ->list:\n",
      "\n",
      "        \"\"\"\n",
      "        Searches through keys and retrieves specified fields from the search results\n",
      "        in the mock database for a given filter.\n",
      "        \"\"\"\n",
      "\n",
      "        if filter_criteria:\n",
      "            self.filter_database(filter_criteria=filter_criteria)\n",
      "\n",
      "        self.search_database_keys(query = query,\n",
      "                                    search_results_n = search_results_n,\n",
      "                                    similarity_search_type = similarity_search_type,\n",
      "                                    similarity_params = similarity_params)\n",
      "\n",
      "        results = self.get_dict_results(return_keys_list = return_keys_list)\n",
      "\n",
      "        # resetting search\n",
      "        self.filtered_data = None\n",
      "        self.keys_list = None\n",
      "\n",
      "        return results\n"
     ]
    }
   ],
   "source": [
    "print(ldh.combine_modules(\n",
    "    # optional\n",
    "    main_module_filepath = \"../tests/package_auto_assembler/mock_vector_database.py\",\n",
    "    dependencies_dir = \"../tests/package_auto_assembler/dependancies/\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

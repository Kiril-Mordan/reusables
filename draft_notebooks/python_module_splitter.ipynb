{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Module Splitter and Analyzer\n",
    "\n",
    "This module provides functionality to analyze and split Python code files into\n",
    "smaller segments based on class definitions. It leverages the Abstract Syntax Tree (AST)\n",
    "for parsing and analysis, allowing for detailed inspection of function definitions, call chains,\n",
    "and class structures within the module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from draft_modules.python_module_splitter import PythonModuleSplitter, PythonCodeAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Call Chains:\n",
      "__init__ calls: None\n",
      "embed_sentence_transformer calls: None\n",
      "embed calls: embed_sentence_transformer\n",
      "embed_texts_in_batches calls: None\n",
      "embed_sentences_in_batches_parallel calls: None\n",
      "__attrs_post_init__ calls: _initialize_logger\n",
      "_initialize_logger calls: None\n",
      "hnsw_search calls: None\n",
      "linear_search calls: None\n",
      "search calls: linear_search, hnsw_search\n",
      "establish_connection calls: None\n",
      "save_data calls: None\n",
      "hash_string_sha256 calls: None\n",
      "_make_key calls: hash_string_sha256\n",
      "_make_embs_key calls: hash_string_sha256\n",
      "_insert_values_dict calls: save_data, _make_embs_key\n",
      "insert_values calls: _make_key, _insert_values_dict\n",
      "flush_database calls: save_data\n",
      "filter_keys calls: None\n",
      "filter_database calls: None\n",
      "remove_from_database calls: save_data\n",
      "search_database_keys calls: _make_embs_key\n",
      "get_dict_results calls: None\n",
      "search_database calls: search_database_keys, get_dict_results, filter_database\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "analyzer = PythonCodeAnalyzer(\"../python_modules/mocker_db.py\")\n",
    "analyzer.parse_file()\n",
    "analyzer.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SentenceTransformerEmbedder': {'__init__': [],\n",
       "  'embed_sentence_transformer': [],\n",
       "  'embed': ['SentenceTransformerEmbedder.embed_sentence_transformer'],\n",
       "  'embed_texts_in_batches': [],\n",
       "  'embed_sentences_in_batches_parallel': []},\n",
       " 'MockerSimilaritySearch': {'__attrs_post_init__': [],\n",
       "  '_initialize_logger': [],\n",
       "  'hnsw_search': [],\n",
       "  'linear_search': [],\n",
       "  'search': ['MockerSimilaritySearch.linear_search',\n",
       "   'MockerSimilaritySearch.hnsw_search']},\n",
       " 'MockerDB': {'__attrs_post_init__': ['MockerDB._initialize_logger'],\n",
       "  '_initialize_logger': [],\n",
       "  'establish_connection': [],\n",
       "  'save_data': [],\n",
       "  'hash_string_sha256': [],\n",
       "  '_make_key': ['MockerDB.hash_string_sha256'],\n",
       "  '_make_embs_key': ['MockerDB.hash_string_sha256'],\n",
       "  '_insert_values_dict': ['MockerDB.save_data', 'MockerDB._make_embs_key'],\n",
       "  'insert_values': ['MockerDB._make_key', 'MockerDB._insert_values_dict'],\n",
       "  'flush_database': ['MockerDB.save_data'],\n",
       "  'filter_keys': [],\n",
       "  'filter_database': [],\n",
       "  'remove_from_database': ['MockerDB.save_data'],\n",
       "  'search_database_keys': ['MockerDB._make_embs_key'],\n",
       "  'get_dict_results': [],\n",
       "  'search_database': ['MockerDB.search_database_keys',\n",
       "   'MockerDB.get_dict_results',\n",
       "   'MockerDB.filter_database']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.class_function_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SentenceTransformerEmbedder': ['__init__',\n",
       "  'embed_sentence_transformer',\n",
       "  'embed',\n",
       "  'embed_texts_in_batches',\n",
       "  'embed_sentences_in_batches_parallel'],\n",
       " 'MockerSimilaritySearch': ['__attrs_post_init__',\n",
       "  '_initialize_logger',\n",
       "  'hnsw_search',\n",
       "  'linear_search',\n",
       "  'search'],\n",
       " 'MockerDB': ['__attrs_post_init__',\n",
       "  '_initialize_logger',\n",
       "  'establish_connection',\n",
       "  'save_data',\n",
       "  'hash_string_sha256',\n",
       "  '_make_key',\n",
       "  '_make_embs_key',\n",
       "  '_insert_values_dict',\n",
       "  'insert_values',\n",
       "  'flush_database',\n",
       "  'filter_keys',\n",
       "  'filter_database',\n",
       "  'remove_from_database',\n",
       "  'search_database_keys',\n",
       "  'get_dict_results',\n",
       "  'search_database']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__init__': [],\n",
       " 'embed_sentence_transformer': [],\n",
       " 'embed': ['embed_sentence_transformer'],\n",
       " 'embed_texts_in_batches': [],\n",
       " 'embed_sentences_in_batches_parallel': [],\n",
       " '__attrs_post_init__': ['_initialize_logger'],\n",
       " '_initialize_logger': [],\n",
       " 'hnsw_search': [],\n",
       " 'linear_search': [],\n",
       " 'search': ['linear_search', 'hnsw_search'],\n",
       " 'establish_connection': [],\n",
       " 'save_data': [],\n",
       " 'hash_string_sha256': [],\n",
       " '_make_key': ['hash_string_sha256'],\n",
       " '_make_embs_key': ['hash_string_sha256'],\n",
       " '_insert_values_dict': ['save_data', '_make_embs_key'],\n",
       " 'insert_values': ['_make_key', '_insert_values_dict'],\n",
       " 'flush_database': ['save_data'],\n",
       " 'filter_keys': [],\n",
       " 'filter_database': [],\n",
       " 'remove_from_database': ['save_data'],\n",
       " 'search_database_keys': ['_make_embs_key'],\n",
       " 'get_dict_results': [],\n",
       " 'search_database': ['search_database_keys',\n",
       "  'get_dict_results',\n",
       "  'filter_database']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.call_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def __init__(self, tbatch_size=32, processing_type='batch', max_workers=2, *args, **kwargs):\n",
      "        logging.getLogger('sentence_transformers').setLevel(logging.ERROR)\n",
      "        self.tbatch_size = tbatch_size\n",
      "        self.processing_type = processing_type\n",
      "        self.max_workers = max_workers\n",
      "        self.model = SentenceTransformer(*args, **kwargs)\n",
      "--------------------\n",
      "\n",
      "\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed_sentence_transformer(self, text):\n",
      "        return self.model.encode(text)\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['SentenceTransformerEmbedder.embed_sentence_transformer']\n",
      "\n",
      "Function chain details: \n",
      "SentenceTransformerEmbedder.embed_sentence_transformer -> []\n",
      "\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed(self, text, processing_type: str=None):\n",
      "        if processing_type is None:\n",
      "            processing_type = self.processing_type\n",
      "        if processing_type == 'batch':\n",
      "            return self.embed_texts_in_batches(texts=text)\n",
      "        if processing_type == 'parallel':\n",
      "            return self.embed_sentences_in_batches_parallel(texts=text)\n",
      "        return self.embed_sentence_transformer(text=str(text))\n",
      "--------------------\n",
      "\n",
      "\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed_texts_in_batches(self, texts, batch_size: int=None):\n",
      "        if batch_size is None:\n",
      "            batch_size = self.tbatch_size\n",
      "        embeddings = []\n",
      "        for i in range(0, len(texts), batch_size):\n",
      "            batch = texts[i:i + batch_size]\n",
      "            batch_embeddings = self.model.encode(batch, show_progress_bar=False)\n",
      "            embeddings.extend(batch_embeddings)\n",
      "        return embeddings\n",
      "--------------------\n",
      "\n",
      "\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed_sentences_in_batches_parallel(self, texts, batch_size: int=None, max_workers: int=None):\n",
      "        if batch_size is None:\n",
      "            batch_size = self.tbatch_size\n",
      "        if max_workers is None:\n",
      "            max_workers = self.max_workers\n",
      "        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
      "        embeddings = []\n",
      "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
      "            future_to_batch = {executor.submit(self.embed_sentence_transformer, batch): batch for batch in batches}\n",
      "            for future in concurrent.futures.as_completed(future_to_batch):\n",
      "                embeddings.extend(future.result())\n",
      "        return embeddings\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self._initialize_logger()\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "\n",
      "    def _initialize_logger(self):\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "            self.logger = logger\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "\n",
      "    def hnsw_search(self, search_emb, doc_embs, k=1, space='cosine', ef_search=50, M=16, ef_construction=200):\n",
      "        dim = len(search_emb)\n",
      "        p = hnswlib.Index(space=space, dim=dim)\n",
      "        p.init_index(max_elements=len(doc_embs), ef_construction=ef_construction, M=M)\n",
      "        p.add_items(doc_embs)\n",
      "        p.set_ef(ef_search)\n",
      "        self.hnsw_index = p\n",
      "        (labels, distances) = p.knn_query(search_emb, k=k)\n",
      "        return (labels[0], distances[0])\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "\n",
      "    def linear_search(self, search_emb, doc_embs, k=1, space='cosine'):\n",
      "        if space == 'cosine':\n",
      "            search_emb_norm = search_emb / np.linalg.norm(search_emb)\n",
      "            doc_embs_norm = doc_embs / np.linalg.norm(doc_embs, axis=1)[:, np.newaxis]\n",
      "            distances = np.dot(doc_embs_norm, search_emb_norm.T).flatten()\n",
      "        elif space == 'l2':\n",
      "            distances = np.linalg.norm(doc_embs - search_emb, axis=1)\n",
      "        if space == 'cosine':\n",
      "            labels = np.argsort(-distances)[:k]\n",
      "        else:\n",
      "            labels = np.argsort(distances)[:k]\n",
      "        top_distances = distances[labels]\n",
      "        return (labels, top_distances)\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerSimilaritySearch.linear_search', 'MockerSimilaritySearch.hnsw_search']\n",
      "\n",
      "Function chain details: \n",
      "MockerSimilaritySearch.linear_search -> []\n",
      "MockerSimilaritySearch.hnsw_search -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "\n",
      "    def search(self, query_embedding, data_embeddings, k: int=None, similarity_search_type: str=None, similarity_params: dict=None):\n",
      "        if k is None:\n",
      "            k = self.search_results_n\n",
      "        if similarity_search_type is None:\n",
      "            similarity_search_type = self.similarity_search_type\n",
      "        if similarity_params is None:\n",
      "            similarity_params = self.similarity_params\n",
      "        if similarity_search_type == 'linear':\n",
      "            return self.linear_search(search_emb=query_embedding, doc_embs=data_embeddings, k=k, **similarity_params)\n",
      "        if similarity_search_type == 'hnsw':\n",
      "            return self.hnsw_search(search_emb=query_embedding, doc_embs=data_embeddings, k=k, **similarity_params)\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB._initialize_logger']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB._initialize_logger -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self._initialize_logger()\n",
      "        self.embedder = self.embedder(**self.embedder_params)\n",
      "        self.similarity_search_h = self.similarity_search_h(similarity_search_type=self.similarity_search_type, search_results_n=self.search_results_n, similarity_params=self.similarity_params, logger=self.logger)\n",
      "        self.data = {}\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def _initialize_logger(self):\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "            self.logger = logger\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def establish_connection(self, file_path: str=None, embs_file_path: str=None):\n",
      "        if file_path is None:\n",
      "            file_path = self.file_path\n",
      "        if embs_file_path is None:\n",
      "            embs_file_path = self.embs_file_path\n",
      "        try:\n",
      "            with open(file_path, 'rb') as file:\n",
      "                self.data = dill.load(file)\n",
      "        except FileNotFoundError:\n",
      "            self.data = {}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Error loading data from file: ', e)\n",
      "        try:\n",
      "            with open(embs_file_path, 'rb') as file:\n",
      "                self.embs = dill.load(file)\n",
      "        except FileNotFoundError:\n",
      "            self.embs = {}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Error loading embeddings storage from file: ', e)\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def save_data(self):\n",
      "        if self.persist:\n",
      "            try:\n",
      "                self.logger.debug('Persisting values')\n",
      "                with open(self.file_path, 'wb') as file:\n",
      "                    dill.dump(self.data, file)\n",
      "                with open(self.embs_file_path, 'wb') as file:\n",
      "                    dill.dump(self.embs, file)\n",
      "            except Exception as e:\n",
      "                self.logger.error('Error saving data to file: ', e)\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def hash_string_sha256(self, input_string: str) -> str:\n",
      "        return hashlib.sha256(input_string.encode()).hexdigest()\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.hash_string_sha256']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.hash_string_sha256 -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def _make_key(self, d: dict, embed: bool) -> str:\n",
      "        input_string = json.dumps(d) + str(embed)\n",
      "        return self.hash_string_sha256(input_string)\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.hash_string_sha256']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.hash_string_sha256 -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def _make_embs_key(self, text: str, model: str) -> str:\n",
      "        input_string = str(text) + str(model)\n",
      "        return self.hash_string_sha256(input_string)\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.save_data', 'MockerDB._make_embs_key']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.save_data -> []\n",
      "MockerDB._make_embs_key -> ['MockerDB.hash_string_sha256']\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def _insert_values_dict(self, values_dicts: dict, var_for_embedding_name: str, model_name: str=None, embed: bool=True) -> None:\n",
      "        if embed:\n",
      "            if model_name is None:\n",
      "                model_name = self.embedder_params['model_name_or_path']\n",
      "            list_of_text_to_embed = [values_dicts[insd][var_for_embedding_name] for insd in values_dicts]\n",
      "            list_of_hash_for_embeddings = [self._make_embs_key(text=text, model=model_name) for text in list_of_text_to_embed]\n",
      "            if model_name not in list(self.embs.keys()):\n",
      "                self.embs[model_name] = {}\n",
      "            current_embs_keys = list(self.embs[model_name].keys())\n",
      "            filtered_list_of_text_to_embed = []\n",
      "            existing_list_of_embeddings = []\n",
      "            for (new_hash, new_text) in zip(list_of_hash_for_embeddings, list_of_text_to_embed):\n",
      "                if new_hash is not current_embs_keys:\n",
      "                    filtered_list_of_text_to_embed.append(new_text)\n",
      "                else:\n",
      "                    existing_list_of_embeddings.append(self.embs[model_name][new_hash])\n",
      "            if self.embedder.processing_type in ['parallel', 'batch']:\n",
      "                new_embedded_list_of_text = self.embedder.embed(text=filtered_list_of_text_to_embed)\n",
      "            else:\n",
      "                new_embedded_list_of_text = [self.embedder.embed(text=text_to_embed) for text_to_embed in filtered_list_of_text_to_embed]\n",
      "            embedded_list_of_text = []\n",
      "            for (new_hash, new_text) in zip(list_of_hash_for_embeddings, list_of_text_to_embed):\n",
      "                if new_hash is not current_embs_keys:\n",
      "                    new_embedding = new_embedded_list_of_text.pop(0)\n",
      "                    embedded_list_of_text.append(new_embedding)\n",
      "                    self.embs[model_name][new_hash] = new_embedding\n",
      "                else:\n",
      "                    embedded_list_of_text.append(existing_list_of_embeddings.pop(0))\n",
      "            i = 0\n",
      "            for insd in values_dicts:\n",
      "                values_dicts[insd]['embedding'] = embedded_list_of_text[i]\n",
      "                i = i + 1\n",
      "        self.data.update(values_dicts)\n",
      "        self.save_data()\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB._make_key', 'MockerDB._insert_values_dict']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB._make_key -> ['MockerDB.hash_string_sha256']\n",
      "MockerDB._insert_values_dict -> ['MockerDB.save_data', 'MockerDB._make_embs_key']\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def insert_values(self, values_dict_list: list, var_for_embedding_name: str, embed: bool=True) -> None:\n",
      "        values_dict_list = copy.deepcopy(values_dict_list)\n",
      "        try:\n",
      "            self.logger.debug('Making unique keys')\n",
      "            values_dict_all = {self._make_key(d=d, embed=embed): d for d in values_dict_list}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during making unique keys foir insert dicts!', e)\n",
      "        try:\n",
      "            self.logger.debug('Remove values that already exist')\n",
      "            values_dict_filtered = {key: values_dict_all[key] for key in values_dict_all.keys() if key not in self.data.keys()}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during filtering out existing inserts!', e)\n",
      "        if values_dict_filtered != {}:\n",
      "            try:\n",
      "                self.logger.debug('Inserting values')\n",
      "                self._insert_values_dict(values_dicts=values_dict_filtered, var_for_embedding_name=var_for_embedding_name, embed=embed)\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during inserting list of key-values dictionaries into mock database!', e)\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.save_data']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.save_data -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def flush_database(self):\n",
      "        try:\n",
      "            self.data = {}\n",
      "            self.save_data()\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during flushing mock database', e)\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def filter_keys(self, subkey=None, subvalue=None):\n",
      "        if subkey is not None and subvalue is not None:\n",
      "            self.keys_list = [d for d in self.data if self.data[d][subkey] == subvalue]\n",
      "        else:\n",
      "            self.keys_list = self.data\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def filter_database(self, filter_criteria: dict=None):\n",
      "        self.filtered_data = {key: value for (key, value) in self.data.items() if all((value.get(k) == v for (k, v) in filter_criteria.items()))}\n",
      "        if len(self.filtered_data) == 0:\n",
      "            self.logger.warning('No data was found with applied filters!')\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.save_data']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.save_data -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def remove_from_database(self, filter_criteria: dict=None):\n",
      "        self.data = {key: value for (key, value) in self.data.items() if not all((value.get(k) == v for (k, v) in filter_criteria.items()))}\n",
      "        self.save_data()\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB._make_embs_key']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB._make_embs_key -> ['MockerDB.hash_string_sha256']\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def search_database_keys(self, query: str, search_results_n: int=None, similarity_search_type: str=None, similarity_params: dict=None, perform_similarity_search: bool=None):\n",
      "        try:\n",
      "            model_name = self.embedder_params['model_name_or_path']\n",
      "            query_hash = self._make_embs_key(text=query, model=model_name)\n",
      "            if model_name in list(self.embs.keys()):\n",
      "                if query_hash not in list(self.embs[model_name].keys()):\n",
      "                    query_embedding = self.embedder.embed(query, processing_type='single')\n",
      "                else:\n",
      "                    query_embedding = self.embs[model_name][query_hash]\n",
      "            else:\n",
      "                self.embs[model_name] = {}\n",
      "                query_embedding = self.embedder.embed(query, processing_type='single')\n",
      "                self.embs[model_name][query_hash] = query_embedding\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during embedding search query!', e)\n",
      "        if search_results_n is None:\n",
      "            search_results_n = self.search_results_n\n",
      "        if similarity_search_type is None:\n",
      "            similarity_search_type = self.similarity_search_type\n",
      "        if similarity_params is None:\n",
      "            similarity_params = self.similarity_params\n",
      "        if self.filtered_data is None:\n",
      "            self.filtered_data = self.data\n",
      "        if self.keys_list is None:\n",
      "            self.keys_list = [key for key in self.filtered_data]\n",
      "        if perform_similarity_search is None:\n",
      "            perform_similarity_search = True\n",
      "        if perform_similarity_search:\n",
      "            try:\n",
      "                data_embeddings = np.array([self.filtered_data[d]['embedding'] for d in self.keys_list])\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during extracting search pool embeddings!', e)\n",
      "            try:\n",
      "                if len(data_embeddings) > 0:\n",
      "                    (labels, distances) = self.similarity_search_h.search(query_embedding=query_embedding, data_embeddings=data_embeddings, k=search_results_n, similarity_search_type=similarity_search_type, similarity_params=similarity_params)\n",
      "                    self.results_keys = [self.keys_list[i] for i in labels]\n",
      "                    self.results_dictances = distances\n",
      "                else:\n",
      "                    self.results_keys = []\n",
      "                    self.results_dictances = None\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during extracting results from the mock database!', e)\n",
      "        else:\n",
      "            try:\n",
      "                self.results_keys = [result_key for result_key in self.filtered_data]\n",
      "                self.results_dictances = np.array([0 for _ in self.filtered_data])\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during extracting search pool embeddings!', e)\n",
      "--------------------\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def get_dict_results(self, return_keys_list: list=None) -> list:\n",
      "        if return_keys_list is None:\n",
      "            return_keys_list = self.return_keys_list\n",
      "        results = []\n",
      "        for searched_doc in self.results_keys:\n",
      "            result = {key: self.data[searched_doc].get(key) for key in return_keys_list}\n",
      "            results.append(result)\n",
      "        return results\n",
      "--------------------\n",
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.search_database_keys', 'MockerDB.get_dict_results', 'MockerDB.filter_database']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.search_database_keys -> ['MockerDB._make_embs_key']\n",
      "MockerDB.get_dict_results -> []\n",
      "MockerDB.filter_database -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def search_database(self, query: str, search_results_n: int=None, filter_criteria: dict=None, similarity_search_type: str=None, similarity_params: dict=None, perform_similarity_search: bool=None, return_keys_list: list=None) -> list:\n",
      "        if filter_criteria:\n",
      "            self.filter_database(filter_criteria=filter_criteria)\n",
      "        self.search_database_keys(query=query, search_results_n=search_results_n, similarity_search_type=similarity_search_type, similarity_params=similarity_params, perform_similarity_search=perform_similarity_search)\n",
      "        results = self.get_dict_results(return_keys_list=return_keys_list)\n",
      "        self.filtered_data = None\n",
      "        self.keys_list = None\n",
      "        self.results_keys = []\n",
      "        return results\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "ps = PythonModuleSplitter(\n",
    "    module_path = '../python_modules/mocker_db.py',\n",
    "    class_function_chains = analyzer.class_function_chains,\n",
    "    include_docstrings = False,\n",
    "    include_class_header = False)\n",
    "\n",
    "splits = ps.split_text()\n",
    "\n",
    "for split in splits:\n",
    "    print(split)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function chains: \n",
      "\n",
      " -> ['MockerDB.search_database_keys', 'MockerDB.get_dict_results', 'MockerDB.filter_database']\n",
      "\n",
      "Function chain details: \n",
      "MockerDB.search_database_keys -> ['MockerDB._make_embs_key']\n",
      "MockerDB.get_dict_results -> []\n",
      "MockerDB.filter_database -> []\n",
      "\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "\n",
      "    def search_database(self, query: str, search_results_n: int=None, filter_criteria: dict=None, similarity_search_type: str=None, similarity_params: dict=None, perform_similarity_search: bool=None, return_keys_list: list=None) -> list:\n",
      "        if filter_criteria:\n",
      "            self.filter_database(filter_criteria=filter_criteria)\n",
      "        self.search_database_keys(query=query, search_results_n=search_results_n, similarity_search_type=similarity_search_type, similarity_params=similarity_params, perform_similarity_search=perform_similarity_search)\n",
      "        results = self.get_dict_results(return_keys_list=return_keys_list)\n",
      "        self.filtered_data = None\n",
      "        self.keys_list = None\n",
      "        self.results_keys = []\n",
      "        return results\n"
     ]
    }
   ],
   "source": [
    "print(ps.segments['MockerDB']['search_database'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

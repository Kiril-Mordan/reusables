{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attr\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class PythonModuleSplitter:\n",
    "\n",
    "    \"\"\"\n",
    "    A class designed to split Python modules into smaller parts based on class definitions,\n",
    "    optionally including or excluding docstrings and class headers from the output.\n",
    "\n",
    "    This class is useful for processing and analyzing Python code at a granular level,\n",
    "    especially when working with large modules.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    module_path : str, optional\n",
    "        Path to the Python module file to be split. Default is None.\n",
    "    include_docstrings : bool, optional\n",
    "        Flag to include docstrings in the output. Default is True.\n",
    "    include_class_header : bool, optional\n",
    "        Flag to include the class header in the output. Default is True.\n",
    "    module_content : str\n",
    "        The content of the Python module file as a string. This attribute is initialized after the class is instantiated.\n",
    "    module_content_no_docstring : str\n",
    "        The content of the Python module file as a string, with docstrings removed. This attribute is initialized after the class is instantiated.\n",
    "    logger : logging.Logger, optional\n",
    "        Custom logger for logging information. Default is None, which causes the class to initialize a new logger.\n",
    "    logger_name : str, optional\n",
    "        Name of the logger to be initialized if no custom logger is provided. Default is 'Python Module Splitter'.\n",
    "    loggerLvl : logging.LEVEL, optional\n",
    "        Logging level for the logger. Default is logging.INFO.\n",
    "    logger_format : str, optional\n",
    "        Logging format for the logger. Default is None, which uses the basic logging format.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    module_path = attr.ib(default=None, type = str)\n",
    "    include_docstrings = attr.ib(default = True, type = bool)\n",
    "    include_class_header = attr.ib(default = True, type = bool)\n",
    "\n",
    "    module_content = attr.ib(init=False)\n",
    "    module_content_no_docstring = attr.ib(init=False)\n",
    "\n",
    "    logger = attr.ib(default=None)\n",
    "    logger_name = attr.ib(default='Python Module Splitter')\n",
    "    loggerLvl = attr.ib(default=logging.INFO)\n",
    "    logger_format = attr.ib(default=None)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self._initialize_logger()\n",
    "        if self.module_path:\n",
    "            self.get_module_code()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize a logger for the class instance based on the specified logging level and logger name.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.logger is None:\n",
    "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
    "            logger = logging.getLogger(self.logger_name)\n",
    "            logger.setLevel(self.loggerLvl)\n",
    "\n",
    "            self.logger = logger\n",
    "\n",
    "    def get_module_code(self, module_path : str = None):\n",
    "\n",
    "        if module_path is None:\n",
    "            module_path = self.module_path\n",
    "            return_content = False\n",
    "        else:\n",
    "            return_content = True\n",
    "\n",
    "        if module_path is None:\n",
    "            raise ValueError(\"Module was not provided!\")\n",
    "\n",
    "        with open(module_path, 'r') as file:\n",
    "            module_content = file.read()\n",
    "\n",
    "        self._remove_docstrings(module_content = module_content)\n",
    "\n",
    "        self.module_content = module_content\n",
    "\n",
    "        if return_content:\n",
    "            return module_content\n",
    "\n",
    "    def _remove_docstrings(self, module_content : str = None):\n",
    "\n",
    "        if module_content is None:\n",
    "            module_content = self.module_content\n",
    "\n",
    "        class DocstringRemover(ast.NodeTransformer):\n",
    "            def visit_FunctionDef(self, node):\n",
    "                \"\"\"Strip docstring from a function definition.\"\"\"\n",
    "                self.generic_visit(node)\n",
    "                if ast.get_docstring(node):\n",
    "                    node.body = node.body[1:]\n",
    "                return node\n",
    "\n",
    "            def visit_ClassDef(self, node):\n",
    "                \"\"\"Strip docstring from a class definition.\"\"\"\n",
    "                self.generic_visit(node)\n",
    "                if ast.get_docstring(node):\n",
    "                    node.body = node.body[1:]\n",
    "                return node\n",
    "\n",
    "        parsed_tree = ast.parse(module_content)\n",
    "        docstring_remover = DocstringRemover()\n",
    "        modified_tree = docstring_remover.visit(parsed_tree)\n",
    "        self.module_content_no_docstring =  ast.unparse(modified_tree)\n",
    "\n",
    "    def split_text(self,\n",
    "                   text : str = None,\n",
    "                   include_docstrings: bool = None,\n",
    "                   include_class_header : bool = None):\n",
    "\n",
    "        if include_docstrings is None:\n",
    "            include_docstrings = self.include_docstrings\n",
    "\n",
    "        if include_class_header is None:\n",
    "            include_class_header = self.include_class_header\n",
    "\n",
    "        if text is None:\n",
    "\n",
    "            if include_docstrings:\n",
    "                module_content = self.module_content\n",
    "            else:\n",
    "                module_content = self.module_content_no_docstring\n",
    "        else:\n",
    "            module_content = text\n",
    "\n",
    "\n",
    "        tree = ast.parse(module_content)\n",
    "        class_definitions = [node for node in tree.body if isinstance(node, ast.ClassDef)]\n",
    "\n",
    "        segments = []\n",
    "        for class_def in class_definitions:\n",
    "\n",
    "            if include_class_header:\n",
    "\n",
    "                # Find the first method\n",
    "                first_method_index = next((i for i, n in enumerate(class_def.body) if isinstance(n, ast.FunctionDef)), None)\n",
    "\n",
    "                # If there's no method, continue to next class\n",
    "                if first_method_index is None:\n",
    "                    continue\n",
    "\n",
    "                # Everything before the first method\n",
    "                pre_method_body = class_def.body[:first_method_index]\n",
    "\n",
    "\n",
    "            for method in [n for n in class_def.body if isinstance(n, ast.FunctionDef)]:\n",
    "\n",
    "                body_code = []\n",
    "                if include_class_header:\n",
    "                    # Class body consists of pre-method part and the current method\n",
    "                    body_code = pre_method_body\n",
    "                body_code = body_code + [method]\n",
    "\n",
    "                class_copy = ast.ClassDef(name=class_def.name,\n",
    "                                        bases=class_def.bases,\n",
    "                                        keywords=class_def.keywords,\n",
    "                                        body=body_code,\n",
    "                                        decorator_list=class_def.decorator_list)\n",
    "                class_code = ast.unparse(class_copy)\n",
    "                segments.append(class_code)\n",
    "\n",
    "        return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def __init__(self, tbatch_size=32, processing_type='batch', max_workers=2, *args, **kwargs):\n",
      "        logging.getLogger('sentence_transformers').setLevel(logging.ERROR)\n",
      "        self.tbatch_size = tbatch_size\n",
      "        self.processing_type = processing_type\n",
      "        self.max_workers = max_workers\n",
      "        self.model = SentenceTransformer(*args, **kwargs)\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed_sentence_transformer(self, text):\n",
      "        return self.model.encode(text)\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed(self, text, processing_type: str=None):\n",
      "        if processing_type is None:\n",
      "            processing_type = self.processing_type\n",
      "        if processing_type == 'batch':\n",
      "            return self.embed_texts_in_batches(texts=text)\n",
      "        if processing_type == 'parallel':\n",
      "            return self.embed_sentences_in_batches_parallel(texts=text)\n",
      "        return self.embed_sentence_transformer(text=str(text))\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed_texts_in_batches(self, texts, batch_size: int=None):\n",
      "        if batch_size is None:\n",
      "            batch_size = self.tbatch_size\n",
      "        embeddings = []\n",
      "        for i in range(0, len(texts), batch_size):\n",
      "            batch = texts[i:i + batch_size]\n",
      "            batch_embeddings = self.model.encode(batch, show_progress_bar=False)\n",
      "            embeddings.extend(batch_embeddings)\n",
      "        return embeddings\n",
      "class SentenceTransformerEmbedder:\n",
      "\n",
      "    def embed_sentences_in_batches_parallel(self, texts, batch_size: int=None, max_workers: int=None):\n",
      "        if batch_size is None:\n",
      "            batch_size = self.tbatch_size\n",
      "        if max_workers is None:\n",
      "            max_workers = self.max_workers\n",
      "        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
      "        embeddings = []\n",
      "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
      "            future_to_batch = {executor.submit(self.embed_sentence_transformer, batch): batch for batch in batches}\n",
      "            for future in concurrent.futures.as_completed(future_to_batch):\n",
      "                embeddings.extend(future.result())\n",
      "        return embeddings\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    similarity_search_type = attr.ib(default='linear')\n",
      "    hnsw_index = attr.ib(init=False)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Similarity search')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self._initialize_logger()\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    similarity_search_type = attr.ib(default='linear')\n",
      "    hnsw_index = attr.ib(init=False)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Similarity search')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "\n",
      "    def _initialize_logger(self):\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "            self.logger = logger\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    similarity_search_type = attr.ib(default='linear')\n",
      "    hnsw_index = attr.ib(init=False)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Similarity search')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "\n",
      "    def hnsw_search(self, search_emb, doc_embs, k=1, space='cosine', ef_search=50, M=16, ef_construction=200):\n",
      "        dim = len(search_emb)\n",
      "        p = hnswlib.Index(space=space, dim=dim)\n",
      "        p.init_index(max_elements=len(doc_embs), ef_construction=ef_construction, M=M)\n",
      "        p.add_items(doc_embs)\n",
      "        p.set_ef(ef_search)\n",
      "        self.hnsw_index = p\n",
      "        (labels, distances) = p.knn_query(search_emb, k=k)\n",
      "        return (labels[0], distances[0])\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    similarity_search_type = attr.ib(default='linear')\n",
      "    hnsw_index = attr.ib(init=False)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Similarity search')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "\n",
      "    def linear_search(self, search_emb, doc_embs, k=1, space='cosine'):\n",
      "        if space == 'cosine':\n",
      "            search_emb_norm = search_emb / np.linalg.norm(search_emb)\n",
      "            doc_embs_norm = doc_embs / np.linalg.norm(doc_embs, axis=1)[:, np.newaxis]\n",
      "            distances = np.dot(doc_embs_norm, search_emb_norm.T).flatten()\n",
      "        elif space == 'l2':\n",
      "            distances = np.linalg.norm(doc_embs - search_emb, axis=1)\n",
      "        if space == 'cosine':\n",
      "            labels = np.argsort(-distances)[:k]\n",
      "        else:\n",
      "            labels = np.argsort(distances)[:k]\n",
      "        top_distances = distances[labels]\n",
      "        return (labels, top_distances)\n",
      "@attr.s\n",
      "class MockerSimilaritySearch:\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    similarity_search_type = attr.ib(default='linear')\n",
      "    hnsw_index = attr.ib(init=False)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Similarity search')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "\n",
      "    def search(self, query_embedding, data_embeddings, k: int=None, similarity_search_type: str=None, similarity_params: dict=None):\n",
      "        if k is None:\n",
      "            k = self.search_results_n\n",
      "        if similarity_search_type is None:\n",
      "            similarity_search_type = self.similarity_search_type\n",
      "        if similarity_params is None:\n",
      "            similarity_params = self.similarity_params\n",
      "        if similarity_search_type == 'linear':\n",
      "            return self.linear_search(search_emb=query_embedding, doc_embs=data_embeddings, k=k, **similarity_params)\n",
      "        if similarity_search_type == 'hnsw':\n",
      "            return self.hnsw_search(search_emb=query_embedding, doc_embs=data_embeddings, k=k, **similarity_params)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def __attrs_post_init__(self):\n",
      "        self._initialize_logger()\n",
      "        self.embedder = self.embedder(**self.embedder_params)\n",
      "        self.similarity_search_h = self.similarity_search_h(similarity_search_type=self.similarity_search_type, search_results_n=self.search_results_n, similarity_params=self.similarity_params, logger=self.logger)\n",
      "        self.data = {}\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def _initialize_logger(self):\n",
      "        if self.logger is None:\n",
      "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
      "            logger = logging.getLogger(self.logger_name)\n",
      "            logger.setLevel(self.loggerLvl)\n",
      "            self.logger = logger\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def establish_connection(self, file_path: str=None, embs_file_path: str=None):\n",
      "        if file_path is None:\n",
      "            file_path = self.file_path\n",
      "        if embs_file_path is None:\n",
      "            embs_file_path = self.embs_file_path\n",
      "        try:\n",
      "            with open(file_path, 'rb') as file:\n",
      "                self.data = dill.load(file)\n",
      "        except FileNotFoundError:\n",
      "            self.data = {}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Error loading data from file: ', e)\n",
      "        try:\n",
      "            with open(embs_file_path, 'rb') as file:\n",
      "                self.embs = dill.load(file)\n",
      "        except FileNotFoundError:\n",
      "            self.embs = {}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Error loading embeddings storage from file: ', e)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def save_data(self):\n",
      "        if self.persist:\n",
      "            try:\n",
      "                self.logger.debug('Persisting values')\n",
      "                with open(self.file_path, 'wb') as file:\n",
      "                    dill.dump(self.data, file)\n",
      "                with open(self.embs_file_path, 'wb') as file:\n",
      "                    dill.dump(self.embs, file)\n",
      "            except Exception as e:\n",
      "                self.logger.error('Error saving data to file: ', e)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def hash_string_sha256(self, input_string: str) -> str:\n",
      "        return hashlib.sha256(input_string.encode()).hexdigest()\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def _make_key(self, d: dict, embed: bool) -> str:\n",
      "        input_string = json.dumps(d) + str(embed)\n",
      "        return self.hash_string_sha256(input_string)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def _make_embs_key(self, text: str, model: str) -> str:\n",
      "        input_string = str(text) + str(model)\n",
      "        return self.hash_string_sha256(input_string)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def _insert_values_dict(self, values_dicts: dict, var_for_embedding_name: str, model_name: str=None, embed: bool=True) -> None:\n",
      "        if embed:\n",
      "            if model_name is None:\n",
      "                model_name = self.embedder_params['model_name_or_path']\n",
      "            list_of_text_to_embed = [values_dicts[insd][var_for_embedding_name] for insd in values_dicts]\n",
      "            list_of_hash_for_embeddings = [self._make_embs_key(text=text, model=model_name) for text in list_of_text_to_embed]\n",
      "            if model_name not in list(self.embs.keys()):\n",
      "                self.embs[model_name] = {}\n",
      "            current_embs_keys = list(self.embs[model_name].keys())\n",
      "            filtered_list_of_text_to_embed = []\n",
      "            existing_list_of_embeddings = []\n",
      "            for (new_hash, new_text) in zip(list_of_hash_for_embeddings, list_of_text_to_embed):\n",
      "                if new_hash is not current_embs_keys:\n",
      "                    filtered_list_of_text_to_embed.append(new_text)\n",
      "                else:\n",
      "                    existing_list_of_embeddings.append(self.embs[model_name][new_hash])\n",
      "            if self.embedder.processing_type in ['parallel', 'batch']:\n",
      "                new_embedded_list_of_text = self.embedder.embed(text=filtered_list_of_text_to_embed)\n",
      "            else:\n",
      "                new_embedded_list_of_text = [self.embedder.embed(text=text_to_embed) for text_to_embed in filtered_list_of_text_to_embed]\n",
      "            embedded_list_of_text = []\n",
      "            for (new_hash, new_text) in zip(list_of_hash_for_embeddings, list_of_text_to_embed):\n",
      "                if new_hash is not current_embs_keys:\n",
      "                    new_embedding = new_embedded_list_of_text.pop(0)\n",
      "                    embedded_list_of_text.append(new_embedding)\n",
      "                    self.embs[model_name][new_hash] = new_embedding\n",
      "                else:\n",
      "                    embedded_list_of_text.append(existing_list_of_embeddings.pop(0))\n",
      "            i = 0\n",
      "            for insd in values_dicts:\n",
      "                values_dicts[insd]['embedding'] = embedded_list_of_text[i]\n",
      "                i = i + 1\n",
      "        self.data.update(values_dicts)\n",
      "        self.save_data()\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def insert_values(self, values_dict_list: list, var_for_embedding_name: str, embed: bool=True) -> None:\n",
      "        values_dict_list = copy.deepcopy(values_dict_list)\n",
      "        try:\n",
      "            self.logger.debug('Making unique keys')\n",
      "            values_dict_all = {self._make_key(d=d, embed=embed): d for d in values_dict_list}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during making unique keys foir insert dicts!', e)\n",
      "        try:\n",
      "            self.logger.debug('Remove values that already exist')\n",
      "            values_dict_filtered = {key: values_dict_all[key] for key in values_dict_all.keys() if key not in self.data.keys()}\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during filtering out existing inserts!', e)\n",
      "        if values_dict_filtered != {}:\n",
      "            try:\n",
      "                self.logger.debug('Inserting values')\n",
      "                self._insert_values_dict(values_dicts=values_dict_filtered, var_for_embedding_name=var_for_embedding_name, embed=embed)\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during inserting list of key-values dictionaries into mock database!', e)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def flush_database(self):\n",
      "        try:\n",
      "            self.data = {}\n",
      "            self.save_data()\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during flushing mock database', e)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def filter_keys(self, subkey=None, subvalue=None):\n",
      "        if subkey is not None and subvalue is not None:\n",
      "            self.keys_list = [d for d in self.data if self.data[d][subkey] == subvalue]\n",
      "        else:\n",
      "            self.keys_list = self.data\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def filter_database(self, filter_criteria: dict=None):\n",
      "        self.filtered_data = {key: value for (key, value) in self.data.items() if all((value.get(k) == v for (k, v) in filter_criteria.items()))}\n",
      "        if len(self.filtered_data) == 0:\n",
      "            self.logger.warning('No data was found with applied filters!')\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def remove_from_database(self, filter_criteria: dict=None):\n",
      "        self.data = {key: value for (key, value) in self.data.items() if not all((value.get(k) == v for (k, v) in filter_criteria.items()))}\n",
      "        self.save_data()\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def search_database_keys(self, query: str, search_results_n: int=None, similarity_search_type: str=None, similarity_params: dict=None, perform_similarity_search: bool=None):\n",
      "        try:\n",
      "            model_name = self.embedder_params['model_name_or_path']\n",
      "            query_hash = self._make_embs_key(text=query, model=model_name)\n",
      "            if model_name in list(self.embs.keys()):\n",
      "                if query_hash not in list(self.embs[model_name].keys()):\n",
      "                    query_embedding = self.embedder.embed(query, processing_type='single')\n",
      "                else:\n",
      "                    query_embedding = self.embs[model_name][query_hash]\n",
      "            else:\n",
      "                self.embs[model_name] = {}\n",
      "                query_embedding = self.embedder.embed(query, processing_type='single')\n",
      "                self.embs[model_name][query_hash] = query_embedding\n",
      "        except Exception as e:\n",
      "            self.logger.error('Problem during embedding search query!', e)\n",
      "        if search_results_n is None:\n",
      "            search_results_n = self.search_results_n\n",
      "        if similarity_search_type is None:\n",
      "            similarity_search_type = self.similarity_search_type\n",
      "        if similarity_params is None:\n",
      "            similarity_params = self.similarity_params\n",
      "        if self.filtered_data is None:\n",
      "            self.filtered_data = self.data\n",
      "        if self.keys_list is None:\n",
      "            self.keys_list = [key for key in self.filtered_data]\n",
      "        if perform_similarity_search is None:\n",
      "            perform_similarity_search = True\n",
      "        if perform_similarity_search:\n",
      "            try:\n",
      "                data_embeddings = np.array([self.filtered_data[d]['embedding'] for d in self.keys_list])\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during extracting search pool embeddings!', e)\n",
      "            try:\n",
      "                if len(data_embeddings) > 0:\n",
      "                    (labels, distances) = self.similarity_search_h.search(query_embedding=query_embedding, data_embeddings=data_embeddings, k=search_results_n, similarity_search_type=similarity_search_type, similarity_params=similarity_params)\n",
      "                    self.results_keys = [self.keys_list[i] for i in labels]\n",
      "                    self.results_dictances = distances\n",
      "                else:\n",
      "                    self.results_keys = []\n",
      "                    self.results_dictances = None\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during extracting results from the mock database!', e)\n",
      "        else:\n",
      "            try:\n",
      "                self.results_keys = [result_key for result_key in self.filtered_data]\n",
      "                self.results_dictances = np.array([0 for _ in self.filtered_data])\n",
      "            except Exception as e:\n",
      "                self.logger.error('Problem during extracting search pool embeddings!', e)\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def get_dict_results(self, return_keys_list: list=None) -> list:\n",
      "        if return_keys_list is None:\n",
      "            return_keys_list = self.return_keys_list\n",
      "        results = []\n",
      "        for searched_doc in self.results_keys:\n",
      "            result = {key: self.data[searched_doc].get(key) for key in return_keys_list}\n",
      "            results.append(result)\n",
      "        return results\n",
      "@attr.s\n",
      "class MockerDB:\n",
      "    embedder_params = attr.ib(default={'model_name_or_path': 'paraphrase-multilingual-mpnet-base-v2', 'processing_type': 'batch', 'tbatch_size': 500})\n",
      "    embedder = attr.ib(default=SentenceTransformerEmbedder)\n",
      "    similarity_search_h = attr.ib(default=MockerSimilaritySearch)\n",
      "    return_keys_list = attr.ib(default=[], type=list)\n",
      "    search_results_n = attr.ib(default=3, type=int)\n",
      "    similarity_search_type = attr.ib(default='linear', type=str)\n",
      "    similarity_params = attr.ib(default={'space': 'cosine'}, type=dict)\n",
      "    file_path = attr.ib(default='./mock_persist', type=str)\n",
      "    embs_file_path = attr.ib(default='./mock_embs_persist', type=str)\n",
      "    persist = attr.ib(default=False, type=bool)\n",
      "    embedder_error_tolerance = attr.ib(default=0.0, type=float)\n",
      "    logger = attr.ib(default=None)\n",
      "    logger_name = attr.ib(default='Mock handler')\n",
      "    loggerLvl = attr.ib(default=logging.INFO)\n",
      "    logger_format = attr.ib(default=None)\n",
      "    data = attr.ib(default=None, init=False)\n",
      "    embs = attr.ib(default=None, init=False)\n",
      "    filtered_data = attr.ib(default=None, init=False)\n",
      "    keys_list = attr.ib(default=None, init=False)\n",
      "    results_keys = attr.ib(default=None, init=False)\n",
      "    results_dictances = attr.ib(default=None, init=False)\n",
      "\n",
      "    def search_database(self, query: str, search_results_n: int=None, filter_criteria: dict=None, similarity_search_type: str=None, similarity_params: dict=None, perform_similarity_search: bool=None, return_keys_list: list=None) -> list:\n",
      "        if filter_criteria:\n",
      "            self.filter_database(filter_criteria=filter_criteria)\n",
      "        self.search_database_keys(query=query, search_results_n=search_results_n, similarity_search_type=similarity_search_type, similarity_params=similarity_params, perform_similarity_search=perform_similarity_search)\n",
      "        results = self.get_dict_results(return_keys_list=return_keys_list)\n",
      "        self.filtered_data = None\n",
      "        self.keys_list = None\n",
      "        self.results_keys = []\n",
      "        return results\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "ps = PythonModuleSplitter(module_path = '../python_modules/mocker_db.py',\n",
    "                          include_docstrings = False)\n",
    "\n",
    "splits = ps.split_text()\n",
    "\n",
    "for split in splits:\n",
    "    print(split)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import logging\n",
    "\n",
    "@attr.s\n",
    "class PythonCodeAnalyzer(ast.NodeVisitor):\n",
    "\n",
    "    \"\"\"\n",
    "    A class to analyze Python code files for various aspects such as defined functions,\n",
    "    call chains within those functions, and class definitions. It leverages the AST\n",
    "    (Abstract Syntax Tree) module to parse and visit nodes in the syntax tree of a Python file.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    filename : str\n",
    "        The path to the Python file to be analyzed.\n",
    "    logger : logging.Logger, optional\n",
    "        Custom logger for logging information. If not provided, a new logger will be initialized.\n",
    "    logger_name : str, optional\n",
    "        The name of the logger to use or initialize. Default is 'Python Code Analyzer'.\n",
    "    loggerLvl : logging.Level, optional\n",
    "        The logging level for the logger. Default is logging.INFO.\n",
    "    logger_format : str, optional\n",
    "        The logging format to use for the logger. Default is None, which uses the basic logging format.\n",
    "    defined_functions : set\n",
    "        A set of names of all functions and methods defined in the analyzed file. Populated after file parsing.\n",
    "    call_chains : dict\n",
    "        A dictionary mapping function names to lists of functions they call. Populated after file parsing.\n",
    "    classes : dict\n",
    "        A dictionary mapping class names to lists of their method names. Populated after file parsing.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filename = attr.ib()\n",
    "\n",
    "    logger = attr.ib(default=None)\n",
    "    logger_name = attr.ib(default='Python Code Analyzer')\n",
    "    loggerLvl = attr.ib(default=logging.INFO)\n",
    "    logger_format = attr.ib(default=None)\n",
    "\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self._initialize_logger()\n",
    "\n",
    "        self.defined_functions = set()  # Stores names of defined functions and methods\n",
    "        self.call_chains = {}  # Stores call chains\n",
    "        self.classes = {}\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize a logger for the class instance based on the specified logging level and logger name.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.logger is None:\n",
    "            logging.basicConfig(level=self.loggerLvl, format=self.logger_format)\n",
    "            logger = logging.getLogger(self.logger_name)\n",
    "            logger.setLevel(self.loggerLvl)\n",
    "\n",
    "            self.logger = logger\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "\n",
    "        \"\"\"\n",
    "        Visits FunctionDef nodes in the AST. It registers the function's name, adds it to the set of\n",
    "        defined functions, and tracks the call chain within the function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : ast.FunctionDef\n",
    "            The node representing a function definition in the AST.\n",
    "        \"\"\"\n",
    "\n",
    "        function_name = node.name\n",
    "        self.defined_functions.add(function_name)\n",
    "        self.call_chains[function_name] = self._find_function_calls(node)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "\n",
    "        \"\"\"\n",
    "        Visits ClassDef nodes in the AST. It registers the class's name and stores the names of its methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : ast.ClassDef\n",
    "            The node representing a class definition in the AST.\n",
    "        \"\"\"\n",
    "\n",
    "        class_name = node.name\n",
    "        self.classes[class_name] = []\n",
    "        for item in node.body:\n",
    "            if isinstance(item, ast.FunctionDef):\n",
    "                self.classes[class_name].append(item.name)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def _find_function_calls(self, node):\n",
    "\n",
    "        \"\"\"\n",
    "        Identifies and returns function calls within a given node. It checks if the function called\n",
    "        is defined within the file and adds it to the call chain.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : ast.Node\n",
    "            The node to search for function calls within.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            A list of names of functions called within the node.\n",
    "        \"\"\"\n",
    "\n",
    "        calls = []\n",
    "        for n in ast.walk(node):\n",
    "            if isinstance(n, ast.Call) and isinstance(n.func, (ast.Attribute, ast.Name)):\n",
    "                called_func_name = ''\n",
    "                if isinstance(n.func, ast.Attribute):\n",
    "                    if isinstance(n.func.value, ast.Name) and n.func.value.id == 'self':\n",
    "                        called_func_name = n.func.attr\n",
    "                elif isinstance(n.func, ast.Name):\n",
    "                    called_func_name = n.func.id\n",
    "\n",
    "                if called_func_name in self.defined_functions:\n",
    "                    calls.append(called_func_name)\n",
    "        return calls\n",
    "\n",
    "    def parse_file(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Parses the Python file specified by `filename`, visiting nodes to collect information on\n",
    "        function definitions, call chains, and class definitions.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(self.filename, 'r') as file:\n",
    "            source_code = file.read()\n",
    "        tree = ast.parse(source_code)\n",
    "        self.visit(tree)\n",
    "\n",
    "    def report(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Prints a report of the function call chains and class method definitions found in the file.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Function Call Chains:\")\n",
    "        for func, calls in self.call_chains.items():\n",
    "            print(f\"{func} calls: {', '.join(calls) if calls else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Call Chains:\n",
      "__init__ calls: None\n",
      "embed_sentence_transformer calls: None\n",
      "embed calls: embed_sentence_transformer\n",
      "embed_texts_in_batches calls: None\n",
      "embed_sentences_in_batches_parallel calls: None\n",
      "__attrs_post_init__ calls: _initialize_logger\n",
      "_initialize_logger calls: None\n",
      "hnsw_search calls: None\n",
      "linear_search calls: None\n",
      "search calls: linear_search, hnsw_search\n",
      "establish_connection calls: None\n",
      "save_data calls: None\n",
      "hash_string_sha256 calls: None\n",
      "_make_key calls: hash_string_sha256\n",
      "_make_embs_key calls: hash_string_sha256\n",
      "_insert_values_dict calls: save_data, _make_embs_key\n",
      "insert_values calls: _make_key, _insert_values_dict\n",
      "flush_database calls: save_data\n",
      "filter_keys calls: None\n",
      "filter_database calls: None\n",
      "remove_from_database calls: save_data\n",
      "search_database_keys calls: _make_embs_key\n",
      "get_dict_results calls: None\n",
      "search_database calls: search_database_keys, get_dict_results, filter_database\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "analyzer = PythonCodeAnalyzer(\"../python_modules/mocker_db.py\")\n",
    "analyzer.parse_file()\n",
    "analyzer.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SentenceTransformerEmbedder': ['__init__',\n",
       "  'embed_sentence_transformer',\n",
       "  'embed',\n",
       "  'embed_texts_in_batches',\n",
       "  'embed_sentences_in_batches_parallel'],\n",
       " 'MockerSimilaritySearch': ['__attrs_post_init__',\n",
       "  '_initialize_logger',\n",
       "  'hnsw_search',\n",
       "  'linear_search',\n",
       "  'search'],\n",
       " 'MockerDB': ['__attrs_post_init__',\n",
       "  '_initialize_logger',\n",
       "  'establish_connection',\n",
       "  'save_data',\n",
       "  'hash_string_sha256',\n",
       "  '_make_key',\n",
       "  '_make_embs_key',\n",
       "  '_insert_values_dict',\n",
       "  'insert_values',\n",
       "  'flush_database',\n",
       "  'filter_keys',\n",
       "  'filter_database',\n",
       "  'remove_from_database',\n",
       "  'search_database_keys',\n",
       "  'get_dict_results',\n",
       "  'search_database']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__init__': [],\n",
       " 'embed_sentence_transformer': [],\n",
       " 'embed': ['embed_sentence_transformer'],\n",
       " 'embed_texts_in_batches': [],\n",
       " 'embed_sentences_in_batches_parallel': [],\n",
       " '__attrs_post_init__': ['_initialize_logger'],\n",
       " '_initialize_logger': [],\n",
       " 'hnsw_search': [],\n",
       " 'linear_search': [],\n",
       " 'search': ['linear_search', 'hnsw_search'],\n",
       " 'establish_connection': [],\n",
       " 'save_data': [],\n",
       " 'hash_string_sha256': [],\n",
       " '_make_key': ['hash_string_sha256'],\n",
       " '_make_embs_key': ['hash_string_sha256'],\n",
       " '_insert_values_dict': ['save_data', '_make_embs_key'],\n",
       " 'insert_values': ['_make_key', '_insert_values_dict'],\n",
       " 'flush_database': ['save_data'],\n",
       " 'filter_keys': [],\n",
       " 'filter_database': [],\n",
       " 'remove_from_database': ['save_data'],\n",
       " 'search_database_keys': ['_make_embs_key'],\n",
       " 'get_dict_results': [],\n",
       " 'search_database': ['search_database_keys',\n",
       "  'get_dict_results',\n",
       "  'filter_database']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.call_chains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
